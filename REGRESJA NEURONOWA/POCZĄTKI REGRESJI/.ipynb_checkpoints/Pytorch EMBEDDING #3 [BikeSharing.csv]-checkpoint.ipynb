{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 220%;color:#1155cc\"> Pytorch: EMBEDDING #3 [BikeSharing.csv] REGRESJA\n",
    "    \n",
    "    \n",
    "<span style=\"font-size: 180%;color:red\"> 20.09.2020   NIE WIEM DLACZEGO MODEL SIĘ NIE UCZY   \n",
    "\n",
    "Classification Example using a neural network with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 30 07:46:46 2021\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time() ## pomiar czasu: start pomiaru czasu\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32minstant         int64\n",
      "dteday         object\n",
      "season          int64\n",
      "yr              int64\n",
      "mnth            int64\n",
      "hr              int64\n",
      "holiday         int64\n",
      "weekday         int64\n",
      "workingday      int64\n",
      "weathersit      int64\n",
      "temp          float64\n",
      "atemp         float64\n",
      "hum           float64\n",
      "windspeed     float64\n",
      "casual          int64\n",
      "registered      int64\n",
      "cnt             int64\n",
      "dtype: object\u001b[0m\n",
      "\n",
      "\n",
      "(17379, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/wojciech/Pulpit/11/hour.csv')\n",
    "\n",
    "def green(text):\n",
    "     print('\\033[32m', text, '\\033[0m', sep='') \n",
    "\n",
    "green(df.dtypes)\n",
    "print()\n",
    "print()\n",
    "print(df.shape)\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spis zmiennych:\n",
    "\n",
    "- <span style=\"font-size: 130%;color:#1155cc\"> instant: </span>indeks rekordu\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">dteday:</span> data\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">season:</span> sezon (1: wiosenny, 2: lato, 3: jesień, 4: zima)\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">yr:</span> rok (0: 2011, 1: 2012)\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">mnth:</span> miesiąc (od 1 do 12)\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">hr .: </span>godzina (od 0 do 23)\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">holiday: </span>pogoda jest dniem świątecznym lub nie (pobrane z http://dchr.dc.gov/page/holiday-schedule)\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">weekday: </span>dzień tygodnia\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">workingday: </span>jeśli dzień nie jest ani weekendem, ani dniem wolnym od pracy wynosi 1, w przeciwnym razie wynosi 0.\n",
    "+ <span style=\"font-size: 130%;color:#1155cc\">weathersit:</span>\n",
    "    - 1: Bezchmurnie, Niewiele chmur, Częściowe zachmurzenie, Częściowe zachmurzenie\n",
    "    - 2: Mgła + Pochmurno, Mgła + Przerwane chmury, Mgła + Kilka chmur, Mgła\n",
    "    - 3: lekki śnieg, lekki deszcz + burza z piorunami + rozproszone chmury, lekki deszcz + rozproszone chmury\n",
    "    - 4: ulewny deszcz + lodowe palety + burza z piorunami + mgła, śnieg + mgła\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">temp: </span>Znormalizowana temperatura w stopniach Celsjusza. Wartości są podzielone na 41 (maks.)\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">atemp: </span>Znormalizowana temperatura odczuwania w stopniach Celsjusza. Wartości są podzielone na 50 (maks.)\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">hum: </span>znormalizowana wilgotność. Wartości są podzielone na 100 (maks.)\n",
    "- <span style=\"font-size: 130%;color:#1155cc\">windspeed: </span>znormalizowana prędkość wiatru. Wartości są podzielone na 67 (maks.)\n",
    "- <span style=\"font-size: 150%;background:#ff00ff\"> casual:</span> liczba przypadkowych użytkowników\n",
    "- <span style=\"font-size: 150%;background:#ff00ff\">registered:</span> liczba zarejestrowanych użytkowników\n",
    "- <span style=\"font-size: 130%;color:red\">cnt: liczba wszystkich wypożyczonych rowerów, w tym zarówno zwykłych, jak i zarejestrowanych\n",
    "    \n",
    "    ![obraz.png](attachment:obraz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 130%;color:#1155cc\">Narzedzie do ograniczenia wielkości próby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17379, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac = 1.00, random_state=148)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 110%;color:#1155cc\"> Sprawdzam czy są braki w danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instant       0\n",
       "dteday        0\n",
       "season        0\n",
       "yr            0\n",
       "mnth          0\n",
       "hr            0\n",
       "holiday       0\n",
       "weekday       0\n",
       "workingday    0\n",
       "weathersit    0\n",
       "temp          0\n",
       "atemp         0\n",
       "hum           0\n",
       "windspeed     0\n",
       "casual        0\n",
       "registered    0\n",
       "cnt           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 110%;color:#1155cc\">Dane nie mają braków. Szukam czy nie ma błędów. Zmienne nie mają błędów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>17379.0</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>977.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>8690.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.50164</td>\n",
       "      <td>0.502561</td>\n",
       "      <td>6.537775</td>\n",
       "      <td>11.546752</td>\n",
       "      <td>0.02877</td>\n",
       "      <td>3.003683</td>\n",
       "      <td>0.682721</td>\n",
       "      <td>1.425283</td>\n",
       "      <td>0.496987</td>\n",
       "      <td>0.475775</td>\n",
       "      <td>0.627229</td>\n",
       "      <td>0.190098</td>\n",
       "      <td>35.676218</td>\n",
       "      <td>153.786869</td>\n",
       "      <td>189.463088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>median</td>\n",
       "      <td>8690.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>142.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        instant      dteday   season        yr       mnth         hr  holiday  \\\n",
       "min         1.0  2011-01-01  1.00000  0.000000   1.000000   0.000000  0.00000   \n",
       "max     17379.0  2012-12-31  4.00000  1.000000  12.000000  23.000000  1.00000   \n",
       "mean     8690.0         NaN  2.50164  0.502561   6.537775  11.546752  0.02877   \n",
       "median   8690.0         NaN  3.00000  1.000000   7.000000  12.000000  0.00000   \n",
       "\n",
       "         weekday  workingday  weathersit      temp     atemp       hum  \\\n",
       "min     0.000000    0.000000    1.000000  0.020000  0.000000  0.000000   \n",
       "max     6.000000    1.000000    4.000000  1.000000  1.000000  1.000000   \n",
       "mean    3.003683    0.682721    1.425283  0.496987  0.475775  0.627229   \n",
       "median  3.000000    1.000000    1.000000  0.500000  0.484800  0.630000   \n",
       "\n",
       "        windspeed      casual  registered         cnt  \n",
       "min      0.000000    0.000000    0.000000    1.000000  \n",
       "max      0.850700  367.000000  886.000000  977.000000  \n",
       "mean     0.190098   35.676218  153.786869  189.463088  \n",
       "median   0.194000   17.000000  115.000000  142.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg(['min', 'max', 'mean', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 110%;color:#1155cc\"> Kasuje niepotrzebne dane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df['Unnamed: 0']\n",
    "del df['casual']\n",
    "del df['registered']\n",
    "del df['instant']\n",
    "del df['dteday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season        0\n",
       "yr            0\n",
       "mnth          0\n",
       "hr            0\n",
       "holiday       0\n",
       "weekday       0\n",
       "workingday    0\n",
       "weathersit    0\n",
       "temp          0\n",
       "atemp         0\n",
       "hum           0\n",
       "windspeed     0\n",
       "cnt           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 220%;color:#1155cc\"> Układ kolumn z danymi kategorycznymi i ciągłymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday',\n",
       "       'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Prosta wersja podziału na dyskretne i ciągłe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data.describe(include=[\"object\"]).columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "data.describe(include=[np.number]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Mojej produkcji podziału na dyskretne i ciągłe\n",
    "    \n",
    "    \n",
    "Które z tych funkcji są kategoryczne?\n",
    "\n",
    "Stawiam dwa warunki: \n",
    "\n",
    "1. funkcja jest kategoryczna bo ma format 'object'\n",
    "2. funkcja jest kategoryczna bo ma do 24 unikalnych wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONLY DISCRETE FUNCTION\n",
      "----------------------\n",
      "season --- int64 --- 4\n",
      "yr --- int64 --- 2\n",
      "mnth --- int64 --- 12\n",
      "hr --- int64 --- 24\n",
      "holiday --- int64 --- 2\n",
      "weekday --- int64 --- 7\n",
      "workingday --- int64 --- 2\n",
      "weathersit --- int64 --- 4\n"
     ]
    }
   ],
   "source": [
    "a,b = df.shape     #<- ile mamy kolumn\n",
    "b\n",
    "\n",
    "print('ONLY DISCRETE FUNCTION')\n",
    "print('----------------------')\n",
    "for i in range(0,b):\n",
    "    i = df.columns[i]\n",
    "    f = df[i].dtypes\n",
    "    h = df[i].nunique()\n",
    "    \n",
    "    if f == np.object or h<=24:\n",
    "        print(i,\"---\",f,\"---\",h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday',\n",
       "       'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Robie to ręcznie - może potem zrobie do tego automat\n",
    "    \n",
    "    Ręcznie przyporządkowuje kolumny do kategorii zmiennych dyskretnych (categorycznych) i zmiennych ciągłych (numerycznych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['season','yr','mnth','hr','holiday','weekday','workingday','weathersit']\n",
    "numerical_columns = ['temp','atemp','hum','windspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88    657\n",
       "0.83    630\n",
       "0.94    560\n",
       "0.87    488\n",
       "0.70    430\n",
       "       ... \n",
       "0.13      1\n",
       "0.12      1\n",
       "0.08      1\n",
       "0.10      1\n",
       "0.91      1\n",
       "Name: hum, Length: 89, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hum'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Wskazuje na zmienną wynikową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digitization of text variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season          int64\n",
       "yr              int64\n",
       "mnth            int64\n",
       "hr              int64\n",
       "holiday         int64\n",
       "weekday         int64\n",
       "workingday      int64\n",
       "weathersit      int64\n",
       "temp          float64\n",
       "atemp         float64\n",
       "hum           float64\n",
       "windspeed     float64\n",
       "cnt             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert types for qualitative columns to category. We can do this using the astype () function, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Zmieniam format zmiennych kategorycznych na type: 'category'</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categorical_columns:\n",
    "    df[category] = df[category].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season        category\n",
       "yr            category\n",
       "mnth          category\n",
       "hr            category\n",
       "holiday       category\n",
       "weekday       category\n",
       "workingday    category\n",
       "weathersit    category\n",
       "temp           float64\n",
       "atemp          float64\n",
       "hum            float64\n",
       "windspeed      float64\n",
       "cnt              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Cyfryzacja zmiennych kategorycznych</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did we digitize data in the format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podstawowym celem oddzielania kolumn kategorialnych od kolumn liczbowych jest to, że wartości w kolumnie liczbowej mogą być bezpośrednio wprowadzane do sieci neuronowych. Jednak wartości kolumn skategoryzowanych należy najpierw przekonwertować na typ liczbowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['season',\n",
       " 'yr',\n",
       " 'mnth',\n",
       " 'hr',\n",
       " 'holiday',\n",
       " 'weekday',\n",
       " 'workingday',\n",
       " 'weathersit']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Conversion of categorical variables to Numpy matrix\n",
    "    \n",
    "    robie to ręcznie - może potem zrobie do tego automat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  1, 22,  0,  2,  1,  1],\n",
       "       [ 3,  0, 10, 10,  0,  4,  1,  1],\n",
       "       [ 3,  1,  9,  3,  0,  0,  0,  1],\n",
       "       [ 1,  1,  5,  5,  0,  4,  1,  0],\n",
       "       [ 3,  0, 10, 17,  1,  5,  0,  0],\n",
       "       [ 2,  0,  5,  5,  0,  2,  1,  1],\n",
       "       [ 3,  0, 10,  5,  0,  0,  0,  0],\n",
       "       [ 3,  1,  9,  1,  0,  4,  1,  1],\n",
       "       [ 1,  1,  4, 15,  0,  6,  0,  0],\n",
       "       [ 0,  1,  1,  4,  0,  4,  1,  2]], dtype=int8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = df['season'].cat.codes.values\n",
    "p2 = df['yr'].cat.codes.values\n",
    "p3 = df['mnth'].cat.codes.values\n",
    "p4 = df['hr'].cat.codes.values\n",
    "p5 = df['holiday'].cat.codes.values\n",
    "p6 = df['weekday'].cat.codes.values\n",
    "p7 = df['workingday'].cat.codes.values\n",
    "p8 = df['weathersit'].cat.codes.values\n",
    "\n",
    "\n",
    "NumP_matrix = np.stack([p1,p2,p3,p4,p5,p6,p7,p8], 1)   \n",
    "\n",
    "NumP_matrix[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Odpalam karte graficzną GPU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#device = torch.device('cpu') # obliczenia robie na CPU\n",
    "device = torch.device('cuda') # obliczenia robie na GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automatyczna wersja tego co powyżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Pytorch tensor from the Numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1, 22,  0,  2,  1,  1],\n",
       "        [ 3,  0, 10, 10,  0,  4,  1,  1],\n",
       "        [ 3,  1,  9,  3,  0,  0,  0,  1],\n",
       "        [ 1,  1,  5,  5,  0,  4,  1,  0],\n",
       "        [ 3,  0, 10, 17,  1,  5,  0,  0],\n",
       "        [ 2,  0,  5,  5,  0,  2,  1,  1],\n",
       "        [ 3,  0, 10,  5,  0,  0,  0,  0],\n",
       "        [ 3,  1,  9,  1,  0,  4,  1,  1],\n",
       "        [ 1,  1,  4, 15,  0,  6,  0,  0],\n",
       "        [ 0,  1,  1,  4,  0,  4,  1,  2]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_data = torch.tensor(NumP_matrix, dtype=torch.int64, device=device)\n",
    "categorical_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of DataFrame numeric columns to a Pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3400, 0.3333, 0.7100, 0.1940],\n",
       "        [0.4400, 0.4394, 0.8800, 0.2239],\n",
       "        [0.5000, 0.4848, 0.6300, 0.3284],\n",
       "        [0.4800, 0.4697, 0.8200, 0.0896],\n",
       "        [0.3400, 0.3030, 0.3900, 0.3284]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_data = np.stack([df[col].values for col in numerical_columns], 1)\n",
    "numerical_data = torch.tensor(numerical_data, dtype=torch.float, device=device)\n",
    "numerical_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert result variables to the Pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wojciech/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([100, 117,  79,  34, 310], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.tensor(df[outputs].values).flatten()\n",
    "outputs = torch.tensor(outputs, device=device)\n",
    "outputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's sum up the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_data:  torch.Size([17379, 8])\n",
      "numerical_data:    torch.Size([17379, 4])\n",
      "outputs:           torch.Size([17379])\n"
     ]
    }
   ],
   "source": [
    "print('categorical_data: ',categorical_data.shape)\n",
    "print('numerical_data:   ',numerical_data.shape)\n",
    "print('outputs:          ',outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">settlement</span>\n",
    "Przekształciliśmy nasze kolumny kategorialne na liczbowe, w których unikalną wartość reprezentuje jedna liczba całkowita (cyfryzacja - np. Palacz to 1). Na podstawie takiej kolumny (zmiennej) możemy trenować model, ale jest lepszy sposób ...\n",
    "\n",
    "Lepszym sposobem jest przedstawienie wartości w kolumnie jakościowej jako N-wymiarowego wektora zamiast pojedynczej liczby całkowitej. Ten proces nazywa się osadzaniem. Wektor jest w stanie uchwycić więcej informacji i może znaleźć relacje między różnymi wartościami kategorialnymi w bardziej odpowiedni sposób. Dlatego będziemy przedstawiać wartości w kolumnach kategorialnych w postaci wektorów N-wymiarowych.\n",
    "\n",
    "Musimy zdefiniować rozmiar osadzania (wymiary wektora) dla wszystkich kolumn jakościowych. Nie ma sztywnej zasady dotyczącej liczby wymiarów. Dobrą zasadą definiowania rozmiaru osadzania dla kolumny jest podzielenie liczby unikalnych wartości w kolumnie przez 2 (ale nie więcej niż 50).\n",
    "\n",
    "Poniższy skrypt tworzy krotkę zawierającą liczbę unikalnych wartości i rozmiary wymiarów dla wszystkich kolumn jakościowych (dyskretnych).\n",
    "\n",
    "Zasada jest prosta: w matrycy osadzającej (categorical_embedding_sizes) musi zawsze znajdować się liczba wierszy większa niż zakres zmiennych w liczbie wierszy: dlatego dodałem col_size + 2, to duży zapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 4), (4, 3), (14, 8), (26, 14), (4, 3), (9, 6), (4, 3), (6, 4)]\n"
     ]
    }
   ],
   "source": [
    "categorical_column_sizes = [len(df[column].cat.categories) for column in categorical_columns]\n",
    "categorical_embedding_sizes = [(col_size+2, min(50, (col_size+5)//2)) for col_size in categorical_column_sizes]\n",
    "print(categorical_embedding_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Dzielenie zestawu na szkoleniowy i testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = df['season'].count()\n",
    "test_records = int(total_records * .2)   # to 20% będzie zbiorem tesowym\n",
    "\n",
    "categorical_train_data = categorical_data[:total_records-test_records]\n",
    "categorical_test_data = categorical_data[total_records-test_records:total_records]\n",
    "numerical_train_data = numerical_data[:total_records-test_records]\n",
    "numerical_test_data = numerical_data[total_records-test_records:total_records]\n",
    "train_outputs = outputs[:total_records-test_records]\n",
    "test_outputs = outputs[total_records-test_records:total_records]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby sprawdzić, czy poprawnie podzieliliśmy dane na zestawy treningów i testów, wydrukujmy długości rekordów szkolenia i testów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_train_data:  torch.Size([13904, 8])\n",
      "numerical_train_data:    torch.Size([13904, 4])\n",
      "train_outputs:           torch.Size([13904])\n",
      "----------------------------------------------------\n",
      "categorical_test_data:   torch.Size([3475, 8])\n",
      "numerical_test_data:     torch.Size([3475, 4])\n",
      "test_outputs:            torch.Size([3475])\n"
     ]
    }
   ],
   "source": [
    "print('categorical_train_data: ',categorical_train_data.shape)\n",
    "print('numerical_train_data:   ',numerical_train_data.shape)\n",
    "print('train_outputs:          ', train_outputs.shape)\n",
    "print('----------------------------------------------------')\n",
    "print('categorical_test_data:  ',categorical_test_data.shape)\n",
    "print('numerical_test_data:    ',numerical_test_data.shape)\n",
    "print('test_outputs:           ',test_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzam czy wszędzie są CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1, 22,  0,  2,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_train_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3400, 0.3333, 0.7100, 0.1940]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_train_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 7, 8, 0, 2, 1, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_test_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6200, 0.6212, 0.5700, 0.1343]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_test_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([493], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 130%;color:#1155cc\"> Model sieci neuronwej (rozbudowany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_embedding_sizes:   [(6, 4), (4, 3), (14, 8), (26, 14), (4, 3), (9, 6), (4, 3), (6, 4)]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print('categorical_embedding_sizes:  ',categorical_embedding_sizes)\n",
    "print(numerical_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deklaruje model jako CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categorical_embedding_sizes, numerical_data.shape[1], 2, [200,100,50], p=0.4).to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = FeedForwardNN(emb_dims, no_of_cont=4, lin_layer_sizes=[50, 100],\n",
    "                          output_size=1, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(6, 4)\n",
      "    (1): Embedding(4, 3)\n",
      "    (2): Embedding(14, 8)\n",
      "    (3): Embedding(26, 14)\n",
      "    (4): Embedding(4, 3)\n",
      "    (5): Embedding(9, 6)\n",
      "    (6): Embedding(4, 3)\n",
      "    (7): Embedding(6, 4)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.4, inplace=False)\n",
      "  (batch_norm_num): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=49, out_features=200, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "    (12): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 280%;color:#1155cc\"> Funkcja straty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 280%;color:#1155cc\"> Optymalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "#optimizer = torch.optim.Rprop(model.parameters(), lr=0.05, etas=(0.5, 1.2), step_sizes=(1e-06, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_embedding_sizes:   [(6, 4), (4, 3), (14, 8), (26, 14), (4, 3), (9, 6), (4, 3), (6, 4)]\n",
      "4\n",
      "categorical_train_data:  torch.Size([13904, 8])\n",
      "numerical_train_data:    torch.Size([13904, 4])\n",
      "outputs:                 torch.Size([13904])\n"
     ]
    }
   ],
   "source": [
    "print('categorical_embedding_sizes:  ',categorical_embedding_sizes)\n",
    "print(numerical_data.shape[1])\n",
    "print('categorical_train_data: ',categorical_train_data.shape)\n",
    "print('numerical_train_data:   ',numerical_train_data.shape)\n",
    "print('outputs:                ',train_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(categorical_train_data, numerical_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.80357456\n",
      "epoch:  41 loss: 0.70356631\n",
      "epoch:  81 loss: 0.69130546\n",
      "epoch: 121 loss: 0.68769306\n",
      "epoch: 161 loss: 0.68708599\n",
      "epoch: 201 loss: 0.68723696\n",
      "epoch: 241 loss: 0.68696499\n",
      "epoch: 281 loss: 0.68719018\n",
      "epoch: 321 loss: 0.68730760\n",
      "epoch: 361 loss: 0.68708068\n",
      "epoch: 401 loss: 0.68705457\n",
      "epoch: 441 loss: 0.68732017\n",
      "epoch: 481 loss: 0.68694824\n",
      "epoch: 521 loss: 0.68706292\n",
      "epoch: 561 loss: 0.68694520\n",
      "epoch: 601 loss: 0.68690312\n",
      "epoch: 641 loss: 0.68741846\n",
      "epoch: 681 loss: 0.68693936\n",
      "epoch: 721 loss: 0.68729854\n",
      "epoch: 761 loss: 0.68715101\n",
      "epoch: 801 loss: 0.68697786\n",
      "epoch: 841 loss: 0.68719018\n",
      "epoch: 881 loss: 0.68679070\n",
      "epoch: 921 loss: 0.68690670\n",
      "epoch: 961 loss: 0.68692034\n",
      "epoch: 1000 loss: 0.6866115332\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "aggregated_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(categorical_train_data, numerical_train_data)\n",
    "    \n",
    "    single_loss = loss_function(y_pred, train_outputs)\n",
    "    aggregated_losses.append(single_loss)\n",
    "\n",
    "    if i%40 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), aggregated_losses, color='r')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train_set: 0.68608749\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_val_train = model(categorical_train_data, numerical_train_data)\n",
    "    loss = loss_function( y_val_train, train_outputs)\n",
    "print(f'Loss train_set: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.67494118\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = model(categorical_test_data, numerical_test_data)\n",
    "    loss = loss_function(y_val, test_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIE POWINNO SIĘ PORÓWNYWAĆ y_val (wynik modelu), KTÓRY MA format ZMIENNYCH FLOAD do test_outputs który ma format zero-jedynkowy. \n",
    "\n",
    "Ponieważ ustaliliśmy, że nasza warstwa wyjściowa będzie zawierała 2 neurony, każda prognoza będzie zawierała 2 wartości. Na przykład pierwsze 5 przewidywanych wartości to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([493,   4,  75, 125, 905], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(test_outputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1411, -1.1810],\n",
      "        [-0.7827, -0.8044],\n",
      "        [-0.2126, -0.1944],\n",
      "        [-0.2576, -0.2323],\n",
      "        [ 0.9478,  0.9633]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZMIANA FORMATU WYNIKÓW Z FORMATU FLOAT NA FORMAT BINARNY\n",
    "\n",
    "Celem takich prognoz jest to, że jeśli faktyczny wynik wynosi 0, wartość wskaźnika 0 powinna być wyższa niż wartość wskaźnika 1 i odwrotnie. Możemy uzyskać największy indeks wartości z listy za pomocą następującego skryptu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\"> Najpier kopiuje tensor y_val z CUDA do CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1411, -1.1810]], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.argmax(y_val, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższe równanie zwraca maksymalne wskaźniki wartości wzdłuż osi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_val[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ na liście pierwotnie przewidywanych wyników dla pierwszych pięciu rekordów wartości indeksów zerowych są większe niż wartości w pierwszych indeksach, możemy zobaczyć 0 w pierwszych pięciu wierszach przetworzonych danych wyjściowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\"> Kopiujemy tensor 'test_outputs' z CUDA do CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = test_outputs.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\"> Sprawdzamy jaki jest poziom klasyfikacji modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0  0  0]\n",
      " [10 15  0 ...  0  0  0]\n",
      " [26 17  0 ...  0  0  0]\n",
      " ...\n",
      " [ 1  0  0 ...  0  0  0]\n",
      " [ 0  1  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.01      0.60      0.02        25\n",
      "           2       0.00      0.00      0.00        43\n",
      "           3       0.00      0.00      0.00        45\n",
      "           4       0.00      0.00      0.00        47\n",
      "           5       0.00      0.00      0.00        50\n",
      "           6       0.00      0.00      0.00        53\n",
      "           7       0.00      0.00      0.00        44\n",
      "           8       0.00      0.00      0.00        34\n",
      "           9       0.00      0.00      0.00        21\n",
      "          10       0.00      0.00      0.00        28\n",
      "          11       0.00      0.00      0.00        39\n",
      "          12       0.00      0.00      0.00        29\n",
      "          13       0.00      0.00      0.00        22\n",
      "          14       0.00      0.00      0.00        26\n",
      "          15       0.00      0.00      0.00        16\n",
      "          16       0.00      0.00      0.00        26\n",
      "          17       0.00      0.00      0.00        19\n",
      "          18       0.00      0.00      0.00        17\n",
      "          19       0.00      0.00      0.00        13\n",
      "          20       0.00      0.00      0.00        16\n",
      "          21       0.00      0.00      0.00        18\n",
      "          22       0.00      0.00      0.00         9\n",
      "          23       0.00      0.00      0.00        15\n",
      "          24       0.00      0.00      0.00        15\n",
      "          25       0.00      0.00      0.00        14\n",
      "          26       0.00      0.00      0.00        27\n",
      "          27       0.00      0.00      0.00        14\n",
      "          28       0.00      0.00      0.00        15\n",
      "          29       0.00      0.00      0.00        13\n",
      "          30       0.00      0.00      0.00        10\n",
      "          31       0.00      0.00      0.00        17\n",
      "          32       0.00      0.00      0.00        21\n",
      "          33       0.00      0.00      0.00        15\n",
      "          34       0.00      0.00      0.00        12\n",
      "          35       0.00      0.00      0.00        11\n",
      "          36       0.00      0.00      0.00        16\n",
      "          37       0.00      0.00      0.00        10\n",
      "          38       0.00      0.00      0.00         9\n",
      "          39       0.00      0.00      0.00        10\n",
      "          40       0.00      0.00      0.00        15\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.00      0.00      0.00        11\n",
      "          43       0.00      0.00      0.00         8\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         8\n",
      "          46       0.00      0.00      0.00         9\n",
      "          47       0.00      0.00      0.00        13\n",
      "          48       0.00      0.00      0.00         9\n",
      "          49       0.00      0.00      0.00         8\n",
      "          50       0.00      0.00      0.00        11\n",
      "          51       0.00      0.00      0.00        11\n",
      "          52       0.00      0.00      0.00         4\n",
      "          53       0.00      0.00      0.00         7\n",
      "          54       0.00      0.00      0.00         9\n",
      "          55       0.00      0.00      0.00         6\n",
      "          56       0.00      0.00      0.00        10\n",
      "          57       0.00      0.00      0.00         9\n",
      "          58       0.00      0.00      0.00         6\n",
      "          59       0.00      0.00      0.00        11\n",
      "          60       0.00      0.00      0.00        10\n",
      "          61       0.00      0.00      0.00         7\n",
      "          62       0.00      0.00      0.00        13\n",
      "          63       0.00      0.00      0.00         4\n",
      "          64       0.00      0.00      0.00         8\n",
      "          65       0.00      0.00      0.00         7\n",
      "          66       0.00      0.00      0.00        11\n",
      "          67       0.00      0.00      0.00         9\n",
      "          68       0.00      0.00      0.00         9\n",
      "          69       0.00      0.00      0.00        10\n",
      "          70       0.00      0.00      0.00         9\n",
      "          71       0.00      0.00      0.00        16\n",
      "          72       0.00      0.00      0.00         3\n",
      "          73       0.00      0.00      0.00         4\n",
      "          74       0.00      0.00      0.00         8\n",
      "          75       0.00      0.00      0.00        10\n",
      "          76       0.00      0.00      0.00        10\n",
      "          77       0.00      0.00      0.00         7\n",
      "          78       0.00      0.00      0.00        13\n",
      "          79       0.00      0.00      0.00         8\n",
      "          80       0.00      0.00      0.00         7\n",
      "          81       0.00      0.00      0.00         5\n",
      "          82       0.00      0.00      0.00         3\n",
      "          83       0.00      0.00      0.00         5\n",
      "          84       0.00      0.00      0.00         7\n",
      "          85       0.00      0.00      0.00        10\n",
      "          86       0.00      0.00      0.00         8\n",
      "          87       0.00      0.00      0.00        10\n",
      "          88       0.00      0.00      0.00         7\n",
      "          89       0.00      0.00      0.00        15\n",
      "          90       0.00      0.00      0.00        10\n",
      "          91       0.00      0.00      0.00        11\n",
      "          92       0.00      0.00      0.00        13\n",
      "          93       0.00      0.00      0.00         7\n",
      "          94       0.00      0.00      0.00        18\n",
      "          95       0.00      0.00      0.00         4\n",
      "          96       0.00      0.00      0.00         9\n",
      "          97       0.00      0.00      0.00         7\n",
      "          98       0.00      0.00      0.00         9\n",
      "          99       0.00      0.00      0.00        16\n",
      "         100       0.00      0.00      0.00         4\n",
      "         101       0.00      0.00      0.00         5\n",
      "         102       0.00      0.00      0.00         9\n",
      "         103       0.00      0.00      0.00         9\n",
      "         104       0.00      0.00      0.00        12\n",
      "         105       0.00      0.00      0.00        10\n",
      "         106       0.00      0.00      0.00         8\n",
      "         107       0.00      0.00      0.00         5\n",
      "         108       0.00      0.00      0.00         6\n",
      "         109       0.00      0.00      0.00         6\n",
      "         110       0.00      0.00      0.00         6\n",
      "         111       0.00      0.00      0.00         4\n",
      "         112       0.00      0.00      0.00         5\n",
      "         113       0.00      0.00      0.00         8\n",
      "         114       0.00      0.00      0.00         7\n",
      "         115       0.00      0.00      0.00         6\n",
      "         116       0.00      0.00      0.00        10\n",
      "         117       0.00      0.00      0.00         4\n",
      "         118       0.00      0.00      0.00         5\n",
      "         119       0.00      0.00      0.00         3\n",
      "         120       0.00      0.00      0.00        12\n",
      "         121       0.00      0.00      0.00         5\n",
      "         122       0.00      0.00      0.00         6\n",
      "         123       0.00      0.00      0.00        10\n",
      "         124       0.00      0.00      0.00        14\n",
      "         125       0.00      0.00      0.00         6\n",
      "         126       0.00      0.00      0.00         7\n",
      "         127       0.00      0.00      0.00         8\n",
      "         128       0.00      0.00      0.00         2\n",
      "         129       0.00      0.00      0.00         9\n",
      "         130       0.00      0.00      0.00         9\n",
      "         131       0.00      0.00      0.00         4\n",
      "         132       0.00      0.00      0.00         4\n",
      "         133       0.00      0.00      0.00         5\n",
      "         134       0.00      0.00      0.00         5\n",
      "         135       0.00      0.00      0.00         8\n",
      "         136       0.00      0.00      0.00        10\n",
      "         137       0.00      0.00      0.00         6\n",
      "         138       0.00      0.00      0.00        13\n",
      "         139       0.00      0.00      0.00         6\n",
      "         140       0.00      0.00      0.00         5\n",
      "         141       0.00      0.00      0.00         9\n",
      "         142       0.00      0.00      0.00         8\n",
      "         143       0.00      0.00      0.00         9\n",
      "         144       0.00      0.00      0.00         7\n",
      "         145       0.00      0.00      0.00        13\n",
      "         146       0.00      0.00      0.00         4\n",
      "         147       0.00      0.00      0.00        11\n",
      "         148       0.00      0.00      0.00         6\n",
      "         149       0.00      0.00      0.00         7\n",
      "         150       0.00      0.00      0.00        11\n",
      "         151       0.00      0.00      0.00         8\n",
      "         152       0.00      0.00      0.00         6\n",
      "         153       0.00      0.00      0.00        14\n",
      "         154       0.00      0.00      0.00        11\n",
      "         155       0.00      0.00      0.00         5\n",
      "         156       0.00      0.00      0.00         2\n",
      "         157       0.00      0.00      0.00         4\n",
      "         158       0.00      0.00      0.00         9\n",
      "         159       0.00      0.00      0.00         5\n",
      "         160       0.00      0.00      0.00         9\n",
      "         161       0.00      0.00      0.00         1\n",
      "         162       0.00      0.00      0.00        11\n",
      "         163       0.00      0.00      0.00         7\n",
      "         164       0.00      0.00      0.00         8\n",
      "         165       0.00      0.00      0.00        10\n",
      "         166       0.00      0.00      0.00        10\n",
      "         167       0.00      0.00      0.00         9\n",
      "         168       0.00      0.00      0.00         7\n",
      "         169       0.00      0.00      0.00         4\n",
      "         170       0.00      0.00      0.00         9\n",
      "         171       0.00      0.00      0.00        12\n",
      "         172       0.00      0.00      0.00         7\n",
      "         173       0.00      0.00      0.00         6\n",
      "         174       0.00      0.00      0.00         6\n",
      "         175       0.00      0.00      0.00        11\n",
      "         176       0.00      0.00      0.00         6\n",
      "         177       0.00      0.00      0.00         8\n",
      "         178       0.00      0.00      0.00         8\n",
      "         179       0.00      0.00      0.00         7\n",
      "         180       0.00      0.00      0.00         8\n",
      "         181       0.00      0.00      0.00        11\n",
      "         182       0.00      0.00      0.00        11\n",
      "         183       0.00      0.00      0.00         6\n",
      "         184       0.00      0.00      0.00         6\n",
      "         185       0.00      0.00      0.00         7\n",
      "         186       0.00      0.00      0.00         5\n",
      "         187       0.00      0.00      0.00         6\n",
      "         188       0.00      0.00      0.00        11\n",
      "         189       0.00      0.00      0.00         9\n",
      "         190       0.00      0.00      0.00        10\n",
      "         191       0.00      0.00      0.00         6\n",
      "         192       0.00      0.00      0.00         8\n",
      "         193       0.00      0.00      0.00         6\n",
      "         194       0.00      0.00      0.00         6\n",
      "         195       0.00      0.00      0.00         6\n",
      "         196       0.00      0.00      0.00         7\n",
      "         197       0.00      0.00      0.00         3\n",
      "         198       0.00      0.00      0.00         9\n",
      "         199       0.00      0.00      0.00         9\n",
      "         200       0.00      0.00      0.00         1\n",
      "         201       0.00      0.00      0.00         6\n",
      "         202       0.00      0.00      0.00         7\n",
      "         203       0.00      0.00      0.00         4\n",
      "         204       0.00      0.00      0.00         5\n",
      "         205       0.00      0.00      0.00        14\n",
      "         206       0.00      0.00      0.00         6\n",
      "         207       0.00      0.00      0.00         6\n",
      "         208       0.00      0.00      0.00         5\n",
      "         209       0.00      0.00      0.00         6\n",
      "         210       0.00      0.00      0.00         6\n",
      "         211       0.00      0.00      0.00         8\n",
      "         212       0.00      0.00      0.00         5\n",
      "         213       0.00      0.00      0.00         8\n",
      "         214       0.00      0.00      0.00         8\n",
      "         215       0.00      0.00      0.00         9\n",
      "         216       0.00      0.00      0.00         6\n",
      "         217       0.00      0.00      0.00         4\n",
      "         218       0.00      0.00      0.00         4\n",
      "         219       0.00      0.00      0.00         2\n",
      "         220       0.00      0.00      0.00         3\n",
      "         221       0.00      0.00      0.00         6\n",
      "         222       0.00      0.00      0.00         8\n",
      "         223       0.00      0.00      0.00         4\n",
      "         224       0.00      0.00      0.00        11\n",
      "         225       0.00      0.00      0.00         6\n",
      "         226       0.00      0.00      0.00         6\n",
      "         227       0.00      0.00      0.00         3\n",
      "         228       0.00      0.00      0.00         6\n",
      "         229       0.00      0.00      0.00         4\n",
      "         230       0.00      0.00      0.00        14\n",
      "         231       0.00      0.00      0.00         3\n",
      "         232       0.00      0.00      0.00         4\n",
      "         233       0.00      0.00      0.00         8\n",
      "         234       0.00      0.00      0.00        10\n",
      "         235       0.00      0.00      0.00         7\n",
      "         236       0.00      0.00      0.00         4\n",
      "         237       0.00      0.00      0.00         7\n",
      "         238       0.00      0.00      0.00         8\n",
      "         239       0.00      0.00      0.00         6\n",
      "         240       0.00      0.00      0.00         5\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         3\n",
      "         243       0.00      0.00      0.00         5\n",
      "         244       0.00      0.00      0.00         3\n",
      "         245       0.00      0.00      0.00         7\n",
      "         246       0.00      0.00      0.00         2\n",
      "         247       0.00      0.00      0.00         5\n",
      "         248       0.00      0.00      0.00         8\n",
      "         249       0.00      0.00      0.00         5\n",
      "         250       0.00      0.00      0.00         2\n",
      "         251       0.00      0.00      0.00         4\n",
      "         252       0.00      0.00      0.00         1\n",
      "         253       0.00      0.00      0.00         4\n",
      "         254       0.00      0.00      0.00         5\n",
      "         255       0.00      0.00      0.00         2\n",
      "         256       0.00      0.00      0.00         4\n",
      "         257       0.00      0.00      0.00         7\n",
      "         258       0.00      0.00      0.00         6\n",
      "         259       0.00      0.00      0.00         3\n",
      "         260       0.00      0.00      0.00         4\n",
      "         261       0.00      0.00      0.00         4\n",
      "         262       0.00      0.00      0.00         3\n",
      "         263       0.00      0.00      0.00         6\n",
      "         264       0.00      0.00      0.00         3\n",
      "         265       0.00      0.00      0.00         5\n",
      "         266       0.00      0.00      0.00         2\n",
      "         267       0.00      0.00      0.00         7\n",
      "         268       0.00      0.00      0.00         2\n",
      "         269       0.00      0.00      0.00         4\n",
      "         270       0.00      0.00      0.00         6\n",
      "         271       0.00      0.00      0.00         2\n",
      "         272       0.00      0.00      0.00         7\n",
      "         273       0.00      0.00      0.00         6\n",
      "         274       0.00      0.00      0.00         8\n",
      "         275       0.00      0.00      0.00         5\n",
      "         276       0.00      0.00      0.00         6\n",
      "         277       0.00      0.00      0.00         2\n",
      "         278       0.00      0.00      0.00         3\n",
      "         279       0.00      0.00      0.00         4\n",
      "         280       0.00      0.00      0.00         5\n",
      "         281       0.00      0.00      0.00         8\n",
      "         282       0.00      0.00      0.00         4\n",
      "         283       0.00      0.00      0.00         3\n",
      "         284       0.00      0.00      0.00         6\n",
      "         285       0.00      0.00      0.00         4\n",
      "         286       0.00      0.00      0.00         7\n",
      "         287       0.00      0.00      0.00         4\n",
      "         288       0.00      0.00      0.00         4\n",
      "         289       0.00      0.00      0.00         3\n",
      "         290       0.00      0.00      0.00         6\n",
      "         291       0.00      0.00      0.00         7\n",
      "         292       0.00      0.00      0.00         2\n",
      "         293       0.00      0.00      0.00         4\n",
      "         294       0.00      0.00      0.00         5\n",
      "         295       0.00      0.00      0.00         4\n",
      "         296       0.00      0.00      0.00         4\n",
      "         297       0.00      0.00      0.00         6\n",
      "         298       0.00      0.00      0.00         6\n",
      "         299       0.00      0.00      0.00        10\n",
      "         300       0.00      0.00      0.00         3\n",
      "         301       0.00      0.00      0.00         2\n",
      "         302       0.00      0.00      0.00         7\n",
      "         303       0.00      0.00      0.00         5\n",
      "         304       0.00      0.00      0.00         4\n",
      "         305       0.00      0.00      0.00         1\n",
      "         306       0.00      0.00      0.00         8\n",
      "         307       0.00      0.00      0.00         2\n",
      "         308       0.00      0.00      0.00         7\n",
      "         309       0.00      0.00      0.00         7\n",
      "         310       0.00      0.00      0.00         5\n",
      "         311       0.00      0.00      0.00         4\n",
      "         312       0.00      0.00      0.00         7\n",
      "         313       0.00      0.00      0.00         5\n",
      "         314       0.00      0.00      0.00         3\n",
      "         315       0.00      0.00      0.00         3\n",
      "         316       0.00      0.00      0.00         3\n",
      "         317       0.00      0.00      0.00         4\n",
      "         318       0.00      0.00      0.00         4\n",
      "         319       0.00      0.00      0.00         4\n",
      "         320       0.00      0.00      0.00         2\n",
      "         321       0.00      0.00      0.00         5\n",
      "         322       0.00      0.00      0.00         5\n",
      "         323       0.00      0.00      0.00         5\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         3\n",
      "         326       0.00      0.00      0.00         2\n",
      "         327       0.00      0.00      0.00         5\n",
      "         328       0.00      0.00      0.00         5\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         8\n",
      "         331       0.00      0.00      0.00         5\n",
      "         332       0.00      0.00      0.00         4\n",
      "         333       0.00      0.00      0.00         2\n",
      "         334       0.00      0.00      0.00         5\n",
      "         336       0.00      0.00      0.00         2\n",
      "         337       0.00      0.00      0.00         3\n",
      "         338       0.00      0.00      0.00         5\n",
      "         339       0.00      0.00      0.00         3\n",
      "         340       0.00      0.00      0.00         1\n",
      "         341       0.00      0.00      0.00         5\n",
      "         342       0.00      0.00      0.00         7\n",
      "         343       0.00      0.00      0.00         5\n",
      "         344       0.00      0.00      0.00         1\n",
      "         345       0.00      0.00      0.00         2\n",
      "         346       0.00      0.00      0.00         4\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.00      0.00      0.00         3\n",
      "         349       0.00      0.00      0.00         4\n",
      "         350       0.00      0.00      0.00         2\n",
      "         351       0.00      0.00      0.00         1\n",
      "         352       0.00      0.00      0.00         1\n",
      "         353       0.00      0.00      0.00         2\n",
      "         354       0.00      0.00      0.00         3\n",
      "         355       0.00      0.00      0.00         6\n",
      "         356       0.00      0.00      0.00         5\n",
      "         357       0.00      0.00      0.00         2\n",
      "         358       0.00      0.00      0.00         2\n",
      "         359       0.00      0.00      0.00         3\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         4\n",
      "         362       0.00      0.00      0.00         3\n",
      "         363       0.00      0.00      0.00         7\n",
      "         364       0.00      0.00      0.00         2\n",
      "         365       0.00      0.00      0.00         3\n",
      "         366       0.00      0.00      0.00         2\n",
      "         367       0.00      0.00      0.00         8\n",
      "         368       0.00      0.00      0.00         2\n",
      "         369       0.00      0.00      0.00         3\n",
      "         370       0.00      0.00      0.00         2\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         4\n",
      "         373       0.00      0.00      0.00         3\n",
      "         374       0.00      0.00      0.00         3\n",
      "         375       0.00      0.00      0.00         3\n",
      "         376       0.00      0.00      0.00         4\n",
      "         377       0.00      0.00      0.00         2\n",
      "         378       0.00      0.00      0.00         4\n",
      "         379       0.00      0.00      0.00         1\n",
      "         380       0.00      0.00      0.00         4\n",
      "         381       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         5\n",
      "         383       0.00      0.00      0.00         5\n",
      "         384       0.00      0.00      0.00         2\n",
      "         385       0.00      0.00      0.00         3\n",
      "         386       0.00      0.00      0.00         2\n",
      "         387       0.00      0.00      0.00         3\n",
      "         388       0.00      0.00      0.00         1\n",
      "         389       0.00      0.00      0.00         2\n",
      "         390       0.00      0.00      0.00         3\n",
      "         391       0.00      0.00      0.00         4\n",
      "         392       0.00      0.00      0.00         2\n",
      "         393       0.00      0.00      0.00         2\n",
      "         394       0.00      0.00      0.00         3\n",
      "         395       0.00      0.00      0.00         4\n",
      "         396       0.00      0.00      0.00         2\n",
      "         397       0.00      0.00      0.00         1\n",
      "         398       0.00      0.00      0.00         5\n",
      "         399       0.00      0.00      0.00         1\n",
      "         401       0.00      0.00      0.00         1\n",
      "         402       0.00      0.00      0.00         1\n",
      "         404       0.00      0.00      0.00         6\n",
      "         405       0.00      0.00      0.00         5\n",
      "         406       0.00      0.00      0.00         1\n",
      "         407       0.00      0.00      0.00         2\n",
      "         408       0.00      0.00      0.00         5\n",
      "         409       0.00      0.00      0.00         4\n",
      "         410       0.00      0.00      0.00         5\n",
      "         411       0.00      0.00      0.00         3\n",
      "         412       0.00      0.00      0.00         4\n",
      "         413       0.00      0.00      0.00         4\n",
      "         414       0.00      0.00      0.00         4\n",
      "         416       0.00      0.00      0.00         1\n",
      "         417       0.00      0.00      0.00         2\n",
      "         418       0.00      0.00      0.00         3\n",
      "         419       0.00      0.00      0.00         3\n",
      "         420       0.00      0.00      0.00         3\n",
      "         421       0.00      0.00      0.00         4\n",
      "         422       0.00      0.00      0.00         1\n",
      "         423       0.00      0.00      0.00         3\n",
      "         424       0.00      0.00      0.00         2\n",
      "         425       0.00      0.00      0.00         1\n",
      "         426       0.00      0.00      0.00         1\n",
      "         427       0.00      0.00      0.00         3\n",
      "         428       0.00      0.00      0.00         5\n",
      "         429       0.00      0.00      0.00         1\n",
      "         430       0.00      0.00      0.00         1\n",
      "         431       0.00      0.00      0.00         3\n",
      "         432       0.00      0.00      0.00         3\n",
      "         433       0.00      0.00      0.00         1\n",
      "         434       0.00      0.00      0.00         2\n",
      "         435       0.00      0.00      0.00         2\n",
      "         436       0.00      0.00      0.00         1\n",
      "         438       0.00      0.00      0.00         1\n",
      "         439       0.00      0.00      0.00         1\n",
      "         440       0.00      0.00      0.00         4\n",
      "         441       0.00      0.00      0.00         6\n",
      "         442       0.00      0.00      0.00         1\n",
      "         444       0.00      0.00      0.00         1\n",
      "         445       0.00      0.00      0.00         2\n",
      "         446       0.00      0.00      0.00         1\n",
      "         447       0.00      0.00      0.00         3\n",
      "         449       0.00      0.00      0.00         1\n",
      "         450       0.00      0.00      0.00         2\n",
      "         451       0.00      0.00      0.00         1\n",
      "         452       0.00      0.00      0.00         4\n",
      "         453       0.00      0.00      0.00         2\n",
      "         454       0.00      0.00      0.00         1\n",
      "         455       0.00      0.00      0.00         2\n",
      "         456       0.00      0.00      0.00         7\n",
      "         457       0.00      0.00      0.00         1\n",
      "         458       0.00      0.00      0.00         3\n",
      "         459       0.00      0.00      0.00         1\n",
      "         460       0.00      0.00      0.00         2\n",
      "         461       0.00      0.00      0.00         1\n",
      "         462       0.00      0.00      0.00         1\n",
      "         463       0.00      0.00      0.00         3\n",
      "         464       0.00      0.00      0.00         3\n",
      "         465       0.00      0.00      0.00         1\n",
      "         466       0.00      0.00      0.00         2\n",
      "         467       0.00      0.00      0.00         1\n",
      "         468       0.00      0.00      0.00         3\n",
      "         469       0.00      0.00      0.00         1\n",
      "         470       0.00      0.00      0.00         3\n",
      "         471       0.00      0.00      0.00         1\n",
      "         472       0.00      0.00      0.00         3\n",
      "         473       0.00      0.00      0.00         2\n",
      "         474       0.00      0.00      0.00         1\n",
      "         475       0.00      0.00      0.00         1\n",
      "         476       0.00      0.00      0.00         2\n",
      "         478       0.00      0.00      0.00         2\n",
      "         479       0.00      0.00      0.00         2\n",
      "         480       0.00      0.00      0.00         3\n",
      "         481       0.00      0.00      0.00         1\n",
      "         482       0.00      0.00      0.00         1\n",
      "         483       0.00      0.00      0.00         6\n",
      "         486       0.00      0.00      0.00         1\n",
      "         487       0.00      0.00      0.00         2\n",
      "         488       0.00      0.00      0.00         2\n",
      "         489       0.00      0.00      0.00         2\n",
      "         490       0.00      0.00      0.00         1\n",
      "         491       0.00      0.00      0.00         1\n",
      "         492       0.00      0.00      0.00         5\n",
      "         493       0.00      0.00      0.00         1\n",
      "         494       0.00      0.00      0.00         1\n",
      "         496       0.00      0.00      0.00         1\n",
      "         497       0.00      0.00      0.00         2\n",
      "         498       0.00      0.00      0.00         6\n",
      "         499       0.00      0.00      0.00         2\n",
      "         501       0.00      0.00      0.00         2\n",
      "         502       0.00      0.00      0.00         2\n",
      "         503       0.00      0.00      0.00         2\n",
      "         504       0.00      0.00      0.00         3\n",
      "         505       0.00      0.00      0.00         2\n",
      "         508       0.00      0.00      0.00         3\n",
      "         509       0.00      0.00      0.00         1\n",
      "         511       0.00      0.00      0.00         1\n",
      "         512       0.00      0.00      0.00         2\n",
      "         513       0.00      0.00      0.00         3\n",
      "         514       0.00      0.00      0.00         2\n",
      "         515       0.00      0.00      0.00         1\n",
      "         516       0.00      0.00      0.00         1\n",
      "         517       0.00      0.00      0.00         2\n",
      "         518       0.00      0.00      0.00         2\n",
      "         519       0.00      0.00      0.00         1\n",
      "         520       0.00      0.00      0.00         1\n",
      "         521       0.00      0.00      0.00         1\n",
      "         522       0.00      0.00      0.00         1\n",
      "         524       0.00      0.00      0.00         2\n",
      "         525       0.00      0.00      0.00         1\n",
      "         526       0.00      0.00      0.00         4\n",
      "         528       0.00      0.00      0.00         2\n",
      "         529       0.00      0.00      0.00         3\n",
      "         530       0.00      0.00      0.00         1\n",
      "         531       0.00      0.00      0.00         3\n",
      "         532       0.00      0.00      0.00         1\n",
      "         535       0.00      0.00      0.00         1\n",
      "         536       0.00      0.00      0.00         1\n",
      "         537       0.00      0.00      0.00         2\n",
      "         538       0.00      0.00      0.00         1\n",
      "         539       0.00      0.00      0.00         3\n",
      "         540       0.00      0.00      0.00         1\n",
      "         542       0.00      0.00      0.00         1\n",
      "         543       0.00      0.00      0.00         2\n",
      "         544       0.00      0.00      0.00         2\n",
      "         551       0.00      0.00      0.00         2\n",
      "         554       0.00      0.00      0.00         3\n",
      "         555       0.00      0.00      0.00         2\n",
      "         558       0.00      0.00      0.00         2\n",
      "         559       0.00      0.00      0.00         1\n",
      "         560       0.00      0.00      0.00         1\n",
      "         563       0.00      0.00      0.00         4\n",
      "         564       0.00      0.00      0.00         1\n",
      "         565       0.00      0.00      0.00         2\n",
      "         566       0.00      0.00      0.00         4\n",
      "         567       0.00      0.00      0.00         1\n",
      "         568       0.00      0.00      0.00         2\n",
      "         569       0.00      0.00      0.00         2\n",
      "         570       0.00      0.00      0.00         1\n",
      "         572       0.00      0.00      0.00         1\n",
      "         573       0.00      0.00      0.00         1\n",
      "         576       0.00      0.00      0.00         2\n",
      "         577       0.00      0.00      0.00         1\n",
      "         578       0.00      0.00      0.00         1\n",
      "         579       0.00      0.00      0.00         2\n",
      "         580       0.00      0.00      0.00         1\n",
      "         581       0.00      0.00      0.00         1\n",
      "         582       0.00      0.00      0.00         2\n",
      "         584       0.00      0.00      0.00         2\n",
      "         585       0.00      0.00      0.00         1\n",
      "         586       0.00      0.00      0.00         2\n",
      "         590       0.00      0.00      0.00         3\n",
      "         591       0.00      0.00      0.00         3\n",
      "         593       0.00      0.00      0.00         1\n",
      "         594       0.00      0.00      0.00         2\n",
      "         595       0.00      0.00      0.00         1\n",
      "         597       0.00      0.00      0.00         1\n",
      "         601       0.00      0.00      0.00         2\n",
      "         602       0.00      0.00      0.00         1\n",
      "         605       0.00      0.00      0.00         2\n",
      "         607       0.00      0.00      0.00         2\n",
      "         608       0.00      0.00      0.00         2\n",
      "         611       0.00      0.00      0.00         1\n",
      "         612       0.00      0.00      0.00         1\n",
      "         613       0.00      0.00      0.00         1\n",
      "         614       0.00      0.00      0.00         1\n",
      "         615       0.00      0.00      0.00         1\n",
      "         619       0.00      0.00      0.00         2\n",
      "         620       0.00      0.00      0.00         1\n",
      "         621       0.00      0.00      0.00         1\n",
      "         624       0.00      0.00      0.00         1\n",
      "         625       0.00      0.00      0.00         1\n",
      "         626       0.00      0.00      0.00         3\n",
      "         628       0.00      0.00      0.00         1\n",
      "         630       0.00      0.00      0.00         1\n",
      "         632       0.00      0.00      0.00         2\n",
      "         633       0.00      0.00      0.00         2\n",
      "         634       0.00      0.00      0.00         1\n",
      "         638       0.00      0.00      0.00         1\n",
      "         640       0.00      0.00      0.00         4\n",
      "         642       0.00      0.00      0.00         1\n",
      "         643       0.00      0.00      0.00         1\n",
      "         644       0.00      0.00      0.00         1\n",
      "         646       0.00      0.00      0.00         2\n",
      "         649       0.00      0.00      0.00         2\n",
      "         651       0.00      0.00      0.00         1\n",
      "         654       0.00      0.00      0.00         1\n",
      "         658       0.00      0.00      0.00         1\n",
      "         659       0.00      0.00      0.00         1\n",
      "         664       0.00      0.00      0.00         1\n",
      "         666       0.00      0.00      0.00         1\n",
      "         667       0.00      0.00      0.00         1\n",
      "         668       0.00      0.00      0.00         1\n",
      "         669       0.00      0.00      0.00         1\n",
      "         671       0.00      0.00      0.00         1\n",
      "         673       0.00      0.00      0.00         1\n",
      "         676       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         1\n",
      "         678       0.00      0.00      0.00         1\n",
      "         680       0.00      0.00      0.00         1\n",
      "         682       0.00      0.00      0.00         1\n",
      "         684       0.00      0.00      0.00         1\n",
      "         686       0.00      0.00      0.00         2\n",
      "         689       0.00      0.00      0.00         1\n",
      "         698       0.00      0.00      0.00         1\n",
      "         699       0.00      0.00      0.00         1\n",
      "         700       0.00      0.00      0.00         1\n",
      "         702       0.00      0.00      0.00         1\n",
      "         704       0.00      0.00      0.00         1\n",
      "         707       0.00      0.00      0.00         1\n",
      "         717       0.00      0.00      0.00         1\n",
      "         722       0.00      0.00      0.00         1\n",
      "         723       0.00      0.00      0.00         1\n",
      "         724       0.00      0.00      0.00         1\n",
      "         728       0.00      0.00      0.00         1\n",
      "         729       0.00      0.00      0.00         1\n",
      "         737       0.00      0.00      0.00         1\n",
      "         738       0.00      0.00      0.00         1\n",
      "         740       0.00      0.00      0.00         1\n",
      "         741       0.00      0.00      0.00         1\n",
      "         743       0.00      0.00      0.00         1\n",
      "         744       0.00      0.00      0.00         2\n",
      "         751       0.00      0.00      0.00         1\n",
      "         755       0.00      0.00      0.00         1\n",
      "         757       0.00      0.00      0.00         3\n",
      "         758       0.00      0.00      0.00         1\n",
      "         759       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00         1\n",
      "         774       0.00      0.00      0.00         1\n",
      "         779       0.00      0.00      0.00         2\n",
      "         781       0.00      0.00      0.00         1\n",
      "         782       0.00      0.00      0.00         1\n",
      "         785       0.00      0.00      0.00         1\n",
      "         790       0.00      0.00      0.00         1\n",
      "         792       0.00      0.00      0.00         1\n",
      "         793       0.00      0.00      0.00         1\n",
      "         797       0.00      0.00      0.00         1\n",
      "         808       0.00      0.00      0.00         1\n",
      "         812       0.00      0.00      0.00         1\n",
      "         817       0.00      0.00      0.00         1\n",
      "         823       0.00      0.00      0.00         1\n",
      "         832       0.00      0.00      0.00         1\n",
      "         839       0.00      0.00      0.00         1\n",
      "         842       0.00      0.00      0.00         1\n",
      "         843       0.00      0.00      0.00         1\n",
      "         847       0.00      0.00      0.00         1\n",
      "         862       0.00      0.00      0.00         1\n",
      "         868       0.00      0.00      0.00         1\n",
      "         884       0.00      0.00      0.00         1\n",
      "         886       0.00      0.00      0.00         1\n",
      "         887       0.00      0.00      0.00         1\n",
      "         888       0.00      0.00      0.00         1\n",
      "         897       0.00      0.00      0.00         1\n",
      "         901       0.00      0.00      0.00         1\n",
      "         905       0.00      0.00      0.00         1\n",
      "         922       0.00      0.00      0.00         1\n",
      "         925       0.00      0.00      0.00         1\n",
      "         968       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.00      3475\n",
      "   macro avg       0.00      0.00      0.00      3475\n",
      "weighted avg       0.00      0.00      0.00      3475\n",
      "\n",
      "accuracy_score: 0.004316546762589928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wojciech/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wojciech/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(test_outputs,y_val))\n",
    "print(classification_report(test_outputs,y_val))\n",
    "print('accuracy_score:',accuracy_score(test_outputs, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;background:#ffd966\">We save the whole model</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wojciech/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,'/home/wojciech/Pulpit/3/byk2.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;background:#ffd966\"> We play the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (all_embeddings): ModuleList(\n",
       "    (0): Embedding(6, 4)\n",
       "    (1): Embedding(4, 3)\n",
       "    (2): Embedding(14, 8)\n",
       "    (3): Embedding(26, 14)\n",
       "    (4): Embedding(4, 3)\n",
       "    (5): Embedding(9, 6)\n",
       "    (6): Embedding(4, 3)\n",
       "    (7): Embedding(6, 4)\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (batch_norm_num): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=49, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.4, inplace=False)\n",
       "    (12): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KOT = torch.load('/home/wojciech/Pulpit/3/byk2.pb')\n",
    "KOT.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 190%;background:#d9ead3\">Bierzemy nasze zmienne testowe - które na początku wydzielilismy przed oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-cf290153e199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(X_test,y_test, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Znowu musimy przygotować dane TESTOWE do wrzucenia w model. Model sieci neuronowych jest juz wykonany ale trzeba przygotować dane, niby nowe dane, dane które wydzieliliśmy na początku. To najlepszy sposób na sprawdzenie na ile model jest dobry. To co jest poniżej to tylko kopia działań, które zrobiłem wyżej aby przygotować dane treningowe przed uruchomieniem modelu    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Mojej produkcji podziału na dyskretne i ciągłe\n",
    "    \n",
    "    \n",
    "Które z tych funkcji są kategoryczne?\n",
    "\n",
    "Stawiam dwa warunki: \n",
    "\n",
    "1. funkcja jest kategoryczna bo ma format 'object'\n",
    "2. funkcja jest kategoryczna bo ma do 24 unikalnych wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = df2.shape     #<- ile mamy kolumn\n",
    "b\n",
    "\n",
    "print('ONLY DISCRETE FUNCTION')\n",
    "print('----------------------')\n",
    "for i in range(0,b):\n",
    "    i = df2.columns[i]\n",
    "    f = df2[i].dtypes\n",
    "    h = df2[i].nunique()\n",
    "    \n",
    "    if f == np.object or h<=24:\n",
    "        print(i,\"---\",f,\"---\",h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Robie to ręcznie - może potem zrobie do tego automat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['Gender', 'Hypertension', 'Heart_Disease', 'Ever_Married', 'Type_Of_Work',\n",
    "       'Residence','Smoking_Status']\n",
    "numerical_columns = ['Age_In_Days', 'Avg_Glucose','BMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Age_In_Days'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We determine that the output variable is the 'Stroke' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['Stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digitization of text variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert types for qualitative columns to category. We can do this using the astype () function, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 150%;color:#1155cc\"> Zmieniam format zmiennych kategorycznych type: 'category'</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categorical_columns:\n",
    "    df2[category] = df2[category].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digitization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did we digitize data in the format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic purpose of separating categorical columns from numeric columns is that the values in the numeric column can be directly input into neural networks. However, categorized column values must first be converted to numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of categorical variables to Numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = df2['Gender'].cat.codes.values\n",
    "p2 = df2['Hypertension'].cat.codes.values\n",
    "p3 = df2['Heart_Disease'].cat.codes.values\n",
    "p4 = df2['Ever_Married'].cat.codes.values\n",
    "p5 = df2['Type_Of_Work'].cat.codes.values\n",
    "p6 = df2['Residence'].cat.codes.values\n",
    "p7 = df2['Smoking_Status'].cat.codes.values\n",
    "\n",
    "\n",
    "NumP_matrix = np.stack([p1,p2,p3,p4,p5,p6,p7], 1)   \n",
    "\n",
    "NumP_matrix[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odpalam karte graficzną GPU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#device = torch.device('cpu') # obliczenia robie na CPU\n",
    "device = torch.device('cuda') # obliczenia robie na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Pytorch tensor from the Numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = torch.tensor(NumP_matrix, dtype=torch.int64, device=device)\n",
    "categorical_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of DataFrame numeric columns to a Pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = np.stack([df2[col].values for col in numerical_columns], 1)\n",
    "numerical_data = torch.tensor(numerical_data, dtype=torch.float, device=device)\n",
    "numerical_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert result variables to the Pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.tensor(df2[outputs].values).flatten()\n",
    "outputs = torch.tensor(outputs, device=device)\n",
    "outputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's sum up the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('categorical_data: ',categorical_data.shape)\n",
    "print('numerical_data:   ',numerical_data.shape)\n",
    "print('outputs:          ',outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">settlement</span>\n",
    "Przekształciliśmy nasze kolumny kategorialne na liczbowe, w których unikalną wartość reprezentuje jedna liczba całkowita (cyfryzacja - np. Palacz to 1). Na podstawie takiej kolumny (zmiennej) możemy trenować model, ale jest lepszy sposób ...\n",
    "\n",
    "Lepszym sposobem jest przedstawienie wartości w kolumnie jakościowej jako N-wymiarowego wektora zamiast pojedynczej liczby całkowitej. Ten proces nazywa się osadzaniem. Wektor jest w stanie uchwycić więcej informacji i może znaleźć relacje między różnymi wartościami kategorialnymi w bardziej odpowiedni sposób. Dlatego będziemy przedstawiać wartości w kolumnach kategorialnych w postaci wektorów N-wymiarowych.\n",
    "\n",
    "Musimy zdefiniować rozmiar osadzania (wymiary wektora) dla wszystkich kolumn jakościowych. Nie ma sztywnej zasady dotyczącej liczby wymiarów. Dobrą zasadą definiowania rozmiaru osadzania dla kolumny jest podzielenie liczby unikalnych wartości w kolumnie przez 2 (ale nie więcej niż 50).\n",
    "\n",
    "Poniższy skrypt tworzy krotkę zawierającą liczbę unikalnych wartości i rozmiary wymiarów dla wszystkich kolumn jakościowych (dyskretnych).\n",
    "\n",
    "Zasada jest prosta: w matrycy osadzającej (categorical_embedding_sizes) musi zawsze znajdować się liczba wierszy większa niż zakres zmiennych w liczbie wierszy: dlatego dodałem col_size + 2, to duży zapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_column_sizes = [len(df2[column].cat.categories) for column in categorical_columns]\n",
    "categorical_embedding_sizes = [(col_size+2, min(50, (col_size+5)//2)) for col_size in categorical_column_sizes]\n",
    "print(categorical_embedding_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = categorical_data\n",
    "A[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = numerical_data\n",
    "B[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 190%;background:#d9ead3\"> Teraz podstawiamy te wartości do wgranego modelu KOT\n",
    "    \n",
    "    wynik modelu to y_pred_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_AB = KOT(A, B)\n",
    "y_pred_AB[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_val_AB = KOT(A,B)\n",
    "    loss = loss_function( y_val_AB, y)\n",
    "print(f'Loss train_set: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "przerabiamy z CUDA na CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_AB = y_val_AB.cpu().data.numpy()\n",
    "y = y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_AB = np.argmax(y_val_AB, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y,y_val_AB))\n",
    "print(classification_report(y,y_val_AB))\n",
    "print(\"Accuracy_score:\" ,accuracy_score(y, y_val_AB))"
   ]
  },
  {
   "attachments": {
    "obraz.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 100%;color:#999999\">loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "<span style=\"font-size: 100%;color:#999999\">loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "<span style=\"font-size: 100%;color:#999999\">optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "<span style=\"font-size: 100%;color:#999999\">optimizer = torch.optim.Rprop(model.parameters(), lr=0.001, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n",
    "\n",
    "![obraz.png](attachment:obraz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Measuring the time to complete this task:')\n",
    "print((time.time() - start_time)/60) ## koniec pomiaru czasu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
