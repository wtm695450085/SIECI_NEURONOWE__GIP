{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 280%;color:#1155cc\"> PyTorch_1_Najprostrza regresja liniowa\n",
    "    \n",
    "<span style=\"font-size: 150%;color:Red\"> 16.09.2020\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\"> Odpalam karte graficzną GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device('cpu') # obliczenia robie na CPU\n",
    "device = torch.device('cuda') # obliczenia robie na GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\">1. Pobranie baza danych csv</span>\n",
    "\n",
    "\n",
    "Zacznijmy od utworzenia przykładowych danych za pomocą torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Happiness Rank</th>\n",
       "      <th>Happiness Score</th>\n",
       "      <th>Economy (GDP per Capita)</th>\n",
       "      <th>Family</th>\n",
       "      <th>Health (Life Expectancy)</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Trust (Government Corruption)</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Dystopia Residual</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>153.0</td>\n",
       "      <td>3.575</td>\n",
       "      <td>0.31982</td>\n",
       "      <td>0.30285</td>\n",
       "      <td>0.30335</td>\n",
       "      <td>0.23414</td>\n",
       "      <td>0.09719</td>\n",
       "      <td>0.36510</td>\n",
       "      <td>1.95210</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Central and Eastern Europe</td>\n",
       "      <td>95.0</td>\n",
       "      <td>4.959</td>\n",
       "      <td>0.87867</td>\n",
       "      <td>0.80434</td>\n",
       "      <td>0.81325</td>\n",
       "      <td>0.35733</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.14272</td>\n",
       "      <td>1.89894</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>68.0</td>\n",
       "      <td>5.605</td>\n",
       "      <td>0.93929</td>\n",
       "      <td>1.07772</td>\n",
       "      <td>0.61766</td>\n",
       "      <td>0.28579</td>\n",
       "      <td>0.17383</td>\n",
       "      <td>0.07822</td>\n",
       "      <td>2.43209</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0      Country                           Region  Happiness Rank  \\\n",
       "0          0  Afghanistan                    Southern Asia           153.0   \n",
       "1          1      Albania       Central and Eastern Europe            95.0   \n",
       "2          2      Algeria  Middle East and Northern Africa            68.0   \n",
       "\n",
       "   Happiness Score  Economy (GDP per Capita)   Family  \\\n",
       "0            3.575                   0.31982  0.30285   \n",
       "1            4.959                   0.87867  0.80434   \n",
       "2            5.605                   0.93929  1.07772   \n",
       "\n",
       "   Health (Life Expectancy)  Freedom  Trust (Government Corruption)  \\\n",
       "0                   0.30335  0.23414                        0.09719   \n",
       "1                   0.81325  0.35733                        0.06413   \n",
       "2                   0.61766  0.28579                        0.17383   \n",
       "\n",
       "   Generosity  Dystopia Residual    Year  \n",
       "0     0.36510            1.95210  2015.0  \n",
       "1     0.14272            1.89894  2015.0  \n",
       "2     0.07822            2.43209  2015.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/media/wojciech/D6DE33C1DE3399271/1A/WorldHappinessReport.csv')\n",
    "# usecols=[1,2,3,4,5]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\">2. Usuwanie pustych komórek NaN </span>\n",
    "\n",
    "Sieci nieuronowe nie lubiś pustych komórek NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                       0\n",
       "Country                          0\n",
       "Region                           0\n",
       "Happiness Rank                   0\n",
       "Happiness Score                  0\n",
       "Economy (GDP per Capita)         0\n",
       "Family                           0\n",
       "Health (Life Expectancy)         0\n",
       "Freedom                          0\n",
       "Trust (Government Corruption)    0\n",
       "Generosity                       0\n",
       "Dystopia Residual                0\n",
       "Year                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(how ='any')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 120%;color:#1155cc\"> Wybieram zmienne do modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Country', 'Region', 'Happiness Rank', 'Happiness Score',\n",
       "       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n",
       "       'Freedom', 'Trust (Government Corruption)', 'Generosity',\n",
       "       'Dystopia Residual', 'Year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Happiness Score','Economy (GDP per Capita)','Freedom', 'Trust (Government Corruption)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Economy (GDP per Capita)</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Happiness Score</th>\n",
       "      <th>Trust (Government Corruption)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.31982</td>\n",
       "      <td>0.23414</td>\n",
       "      <td>3.575</td>\n",
       "      <td>0.09719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.87867</td>\n",
       "      <td>0.35733</td>\n",
       "      <td>4.959</td>\n",
       "      <td>0.06413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.93929</td>\n",
       "      <td>0.28579</td>\n",
       "      <td>5.605</td>\n",
       "      <td>0.17383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Economy (GDP per Capita)  Freedom  Happiness Score  \\\n",
       "0                   0.31982  0.23414            3.575   \n",
       "1                   0.87867  0.35733            4.959   \n",
       "2                   0.93929  0.28579            5.605   \n",
       "\n",
       "   Trust (Government Corruption)  \n",
       "0                        0.09719  \n",
       "1                        0.06413  \n",
       "2                        0.17383  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Economy (GDP per Capita)','Freedom','Happiness Score', 'Trust (Government Corruption)']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\">3. Przekształcanie na tensor zmiennych niezależnych</span>\n",
    "\n",
    " - 'Happiness Score',\n",
    " - 'Economy (GDP per Capita)',\n",
    " - 'Freedom', \n",
    " - 'Trust (Government Corruption)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3198, 0.8787, 0.9393,  ..., 0.5917, 0.6364, 0.3758],\n",
       "        [0.2341, 0.3573, 0.2858,  ..., 0.2495, 0.4616, 0.3364],\n",
       "        [0.0972, 0.0641, 0.1738,  ..., 0.0568, 0.0782, 0.0954]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor((df['Economy (GDP per Capita)'],df['Freedom'],df['Trust (Government Corruption)']), dtype=torch.float, device=device)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak widać powyżej kolumny odpowiadają wierszom z dataframe dlatego\n",
    "\n",
    "\n",
    "<span style=\"font-size: 180%;color:#1155cc\"> 3.1 TRansponuje wektor zmiennych niezależnych aby stał się kolumną</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0972, 0.2341, 0.3198],\n",
       "        [0.0641, 0.3573, 0.8787],\n",
       "        [0.1738, 0.2858, 0.9393],\n",
       "        ...,\n",
       "        [0.0568, 0.2495, 0.5917],\n",
       "        [0.0782, 0.4616, 0.6364],\n",
       "        [0.0954, 0.3364, 0.3758]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.transpose(X.flip(0),0,1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma mieć 469 obserwacje dla trzech zmiennych opisujących: 'Trust (Government Corruption)','Freedom', 'Economy (GDP per Capita)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([469, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\">4. Przekształcanie na tensor zmiennych zależnych: 'Happiness Score'</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([469])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.5750, 4.9590, 5.6050, 4.0330, 6.5740, 4.3500, 7.2840, 7.2000, 5.2120,\n",
       "        5.9600, 4.6940, 5.8130, 6.9370, 3.3400, 5.2530, 5.8900, 4.9490, 4.3320,\n",
       "        6.9830, 4.2180, 3.5870, 2.9050, 3.8190, 4.2520, 7.4270, 3.6780, 3.6670,\n",
       "        6.6700, 5.1400, 6.4770, 3.9560, 3.9890, 4.5170, 7.2260, 5.7590, 5.6890,\n",
       "        6.5050, 7.5270, 4.3690, 4.8850, 5.9750, 4.1940, 6.1300, 5.4290, 4.5120,\n",
       "        7.4060, 6.5750, 3.8960, 4.2970, 6.7500, 4.6330, 4.8570, 6.1230, 3.6560,\n",
       "        4.5180, 4.7880, 5.4740, 4.8000, 7.5610, 4.5650, 5.3990, 4.6860, 4.6770,\n",
       "        6.9400, 7.2780, 5.9480, 3.6550, 5.7090, 5.9870, 5.1920, 5.8550, 4.4190,\n",
       "        5.5890, 6.2950, 5.2860, 4.8760, 5.0980, 4.8390, 4.8980, 4.5710, 5.7540,\n",
       "        5.8330, 6.9460, 5.0070, 3.6810, 4.2920, 5.7700, 3.9950, 6.3020, 4.4360,\n",
       "        5.4770, 7.1870, 5.8890, 4.8740, 5.1920, 5.0130, 4.9710, 4.3070, 4.5140,\n",
       "        7.3780, 7.2860, 5.8280, 3.8450, 5.2680, 5.6950, 7.5220, 6.8530, 5.1940,\n",
       "        4.7150, 6.7860, 5.8780, 5.8240, 5.0730, 5.7910, 5.1020, 6.6110, 5.1240,\n",
       "        5.7160, 3.4650, 6.4110, 3.9040, 5.1230, 4.5070, 6.7980, 5.9950, 5.8480,\n",
       "        5.0570, 4.6420, 5.9840, 6.3290, 4.2710, 4.5500, 6.2690, 4.8670, 7.3640,\n",
       "        7.5870, 3.0060, 6.2980, 4.7860, 3.7810, 6.4550, 2.8390, 6.1680, 4.7390,\n",
       "        5.3320, 5.5480, 3.9310, 4.6810, 6.9010, 6.8670, 7.1190, 6.4850, 6.0030,\n",
       "        6.8100, 5.3600, 4.0770, 5.1290, 4.6100, 3.3600, 4.6550, 6.3550, 3.8660,\n",
       "        6.6500, 4.3600, 7.3130, 7.1190, 5.2910, 6.2180, 4.6430, 5.8020, 6.9290,\n",
       "        5.9560, 3.4840, 5.1960, 5.8220, 5.1630, 3.9740, 6.9520, 4.2170, 3.7390,\n",
       "        2.9050, 3.9070, 4.5130, 7.4040, 3.7630, 6.7050, 5.2450, 6.4810, 3.9560,\n",
       "        4.2360, 4.2720, 7.0870, 5.4880, 5.5460, 6.5960, 7.5260, 5.1550, 5.9760,\n",
       "        4.3620, 6.0680, 5.5170, 4.5080, 7.4130, 6.4780, 4.1210, 4.2520, 6.9940,\n",
       "        4.2760, 5.0330, 6.3240, 3.6070, 4.0280, 4.8710, 5.4580, 5.1450, 7.5010,\n",
       "        4.4040, 5.3140, 4.8130, 4.5750, 6.9070, 7.2670, 5.9770, 3.9160, 5.5100,\n",
       "        5.9210, 5.3030, 5.9190, 4.3560, 5.4010, 6.2390, 5.1850, 4.8760, 5.5600,\n",
       "        5.1290, 3.6220, 5.6150, 5.8130, 6.8710, 5.1210, 3.6950, 4.1560, 6.0050,\n",
       "        4.0730, 6.4880, 4.2010, 5.6480, 6.7780, 5.8970, 4.9070, 5.1610, 5.1510,\n",
       "        4.3950, 4.5740, 4.7930, 7.3390, 7.3340, 5.9920, 3.8560, 4.8750, 5.7710,\n",
       "        7.4980, 5.1320, 4.7540, 6.7010, 5.5380, 5.7430, 5.2790, 5.8350, 5.1230,\n",
       "        7.0390, 6.3750, 5.5280, 5.8560, 3.5150, 6.3790, 4.2190, 5.1770, 4.6350,\n",
       "        6.7390, 6.0780, 5.7680, 5.4400, 5.0570, 4.4590, 5.8350, 3.8320, 6.3610,\n",
       "        4.4150, 4.1390, 6.2690, 7.2910, 7.5090, 3.0690, 6.3790, 4.9960, 3.6660,\n",
       "        6.4740, 3.3030, 6.1680, 5.0450, 5.3890, 5.6580, 3.7390, 4.3240, 6.5730,\n",
       "        6.7250, 7.1040, 6.5450, 5.9870, 6.0840, 5.0610, 3.7240, 4.7950, 4.1930,\n",
       "        3.7940, 4.6440, 5.8720, 3.7950, 6.5990, 4.3760, 7.2840, 7.0060, 5.2340,\n",
       "        6.0870, 4.6080, 5.5690, 6.8910, 5.9560, 3.6570, 5.0110, 5.8230, 5.1820,\n",
       "        3.7660, 6.6350, 4.7140, 4.0320, 2.9050, 4.1680, 4.6950, 7.3160, 2.6930,\n",
       "        3.9360, 6.6520, 5.2730, 6.3570, 4.2910, 4.2800, 7.0790, 5.2930, 5.6210,\n",
       "        6.6090, 7.5220, 5.2300, 6.0080, 4.7350, 6.0030, 5.6110, 4.4600, 7.4690,\n",
       "        6.4420, 4.4650, 4.2860, 6.9510, 4.1200, 5.2270, 6.4540, 3.5070, 3.6030,\n",
       "        5.1810, 5.3240, 7.5040, 4.3150, 5.2620, 4.6920, 4.4970, 6.9770, 7.2130,\n",
       "        5.9640, 4.1800, 5.3110, 5.9200, 5.3360, 5.8190, 4.5530, 5.2790, 6.1050,\n",
       "        5.0040, 5.8500, 5.2250, 3.8080, 3.5330, 5.5250, 5.9020, 6.8630, 5.1750,\n",
       "        3.6440, 3.9700, 6.0840, 4.1900, 6.5270, 4.2920, 5.6290, 6.5780, 5.8380,\n",
       "        4.9550, 5.2370, 5.2350, 4.5500, 4.5450, 4.5740, 4.9620, 7.3770, 7.3140,\n",
       "        6.0710, 4.0280, 5.0740, 5.8100, 7.5370, 5.2690, 4.7750, 6.4520, 5.4930,\n",
       "        5.7150, 5.4300, 5.9730, 5.1950, 6.3750, 5.8250, 5.9630, 3.4710, 6.3440,\n",
       "        4.5350, 5.3950, 4.7090, 6.5720, 6.0980, 5.7580, 5.1510, 4.8290, 5.8380,\n",
       "        3.5910, 6.4030, 4.4400, 4.1390, 7.2840, 7.4940, 3.4620, 6.4220, 5.0410,\n",
       "        3.3490, 6.4240, 3.4950, 6.1680, 4.8050, 5.5000, 5.8220, 4.0810, 4.0960,\n",
       "        6.6480, 6.7140, 6.9930, 6.4540, 5.9710, 5.2500, 5.0740, 3.5930, 4.5140,\n",
       "        3.8750], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor((df['Happiness Score']), dtype=torch.float,device=device)\n",
    "\n",
    "print(y.size())\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\"> 4.1 TRansponuje wektor wynikowy aby stał się kolumną</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.view(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5750],\n",
       "        [4.9590],\n",
       "        [5.6050],\n",
       "        [4.0330],\n",
       "        [6.5740],\n",
       "        [4.3500],\n",
       "        [7.2840],\n",
       "        [7.2000],\n",
       "        [5.2120],\n",
       "        [5.9600],\n",
       "        [4.6940],\n",
       "        [5.8130],\n",
       "        [6.9370],\n",
       "        [3.3400],\n",
       "        [5.2530],\n",
       "        [5.8900],\n",
       "        [4.9490],\n",
       "        [4.3320],\n",
       "        [6.9830],\n",
       "        [4.2180],\n",
       "        [3.5870],\n",
       "        [2.9050],\n",
       "        [3.8190],\n",
       "        [4.2520],\n",
       "        [7.4270],\n",
       "        [3.6780],\n",
       "        [3.6670],\n",
       "        [6.6700],\n",
       "        [5.1400],\n",
       "        [6.4770],\n",
       "        [3.9560],\n",
       "        [3.9890],\n",
       "        [4.5170],\n",
       "        [7.2260],\n",
       "        [5.7590],\n",
       "        [5.6890],\n",
       "        [6.5050],\n",
       "        [7.5270],\n",
       "        [4.3690],\n",
       "        [4.8850],\n",
       "        [5.9750],\n",
       "        [4.1940],\n",
       "        [6.1300],\n",
       "        [5.4290],\n",
       "        [4.5120],\n",
       "        [7.4060],\n",
       "        [6.5750],\n",
       "        [3.8960],\n",
       "        [4.2970],\n",
       "        [6.7500],\n",
       "        [4.6330],\n",
       "        [4.8570],\n",
       "        [6.1230],\n",
       "        [3.6560],\n",
       "        [4.5180],\n",
       "        [4.7880],\n",
       "        [5.4740],\n",
       "        [4.8000],\n",
       "        [7.5610],\n",
       "        [4.5650],\n",
       "        [5.3990],\n",
       "        [4.6860],\n",
       "        [4.6770],\n",
       "        [6.9400],\n",
       "        [7.2780],\n",
       "        [5.9480],\n",
       "        [3.6550],\n",
       "        [5.7090],\n",
       "        [5.9870],\n",
       "        [5.1920],\n",
       "        [5.8550],\n",
       "        [4.4190],\n",
       "        [5.5890],\n",
       "        [6.2950],\n",
       "        [5.2860],\n",
       "        [4.8760],\n",
       "        [5.0980],\n",
       "        [4.8390],\n",
       "        [4.8980],\n",
       "        [4.5710],\n",
       "        [5.7540],\n",
       "        [5.8330],\n",
       "        [6.9460],\n",
       "        [5.0070],\n",
       "        [3.6810],\n",
       "        [4.2920],\n",
       "        [5.7700],\n",
       "        [3.9950],\n",
       "        [6.3020],\n",
       "        [4.4360],\n",
       "        [5.4770],\n",
       "        [7.1870],\n",
       "        [5.8890],\n",
       "        [4.8740],\n",
       "        [5.1920],\n",
       "        [5.0130],\n",
       "        [4.9710],\n",
       "        [4.3070],\n",
       "        [4.5140],\n",
       "        [7.3780],\n",
       "        [7.2860],\n",
       "        [5.8280],\n",
       "        [3.8450],\n",
       "        [5.2680],\n",
       "        [5.6950],\n",
       "        [7.5220],\n",
       "        [6.8530],\n",
       "        [5.1940],\n",
       "        [4.7150],\n",
       "        [6.7860],\n",
       "        [5.8780],\n",
       "        [5.8240],\n",
       "        [5.0730],\n",
       "        [5.7910],\n",
       "        [5.1020],\n",
       "        [6.6110],\n",
       "        [5.1240],\n",
       "        [5.7160],\n",
       "        [3.4650],\n",
       "        [6.4110],\n",
       "        [3.9040],\n",
       "        [5.1230],\n",
       "        [4.5070],\n",
       "        [6.7980],\n",
       "        [5.9950],\n",
       "        [5.8480],\n",
       "        [5.0570],\n",
       "        [4.6420],\n",
       "        [5.9840],\n",
       "        [6.3290],\n",
       "        [4.2710],\n",
       "        [4.5500],\n",
       "        [6.2690],\n",
       "        [4.8670],\n",
       "        [7.3640],\n",
       "        [7.5870],\n",
       "        [3.0060],\n",
       "        [6.2980],\n",
       "        [4.7860],\n",
       "        [3.7810],\n",
       "        [6.4550],\n",
       "        [2.8390],\n",
       "        [6.1680],\n",
       "        [4.7390],\n",
       "        [5.3320],\n",
       "        [5.5480],\n",
       "        [3.9310],\n",
       "        [4.6810],\n",
       "        [6.9010],\n",
       "        [6.8670],\n",
       "        [7.1190],\n",
       "        [6.4850],\n",
       "        [6.0030],\n",
       "        [6.8100],\n",
       "        [5.3600],\n",
       "        [4.0770],\n",
       "        [5.1290],\n",
       "        [4.6100],\n",
       "        [3.3600],\n",
       "        [4.6550],\n",
       "        [6.3550],\n",
       "        [3.8660],\n",
       "        [6.6500],\n",
       "        [4.3600],\n",
       "        [7.3130],\n",
       "        [7.1190],\n",
       "        [5.2910],\n",
       "        [6.2180],\n",
       "        [4.6430],\n",
       "        [5.8020],\n",
       "        [6.9290],\n",
       "        [5.9560],\n",
       "        [3.4840],\n",
       "        [5.1960],\n",
       "        [5.8220],\n",
       "        [5.1630],\n",
       "        [3.9740],\n",
       "        [6.9520],\n",
       "        [4.2170],\n",
       "        [3.7390],\n",
       "        [2.9050],\n",
       "        [3.9070],\n",
       "        [4.5130],\n",
       "        [7.4040],\n",
       "        [3.7630],\n",
       "        [6.7050],\n",
       "        [5.2450],\n",
       "        [6.4810],\n",
       "        [3.9560],\n",
       "        [4.2360],\n",
       "        [4.2720],\n",
       "        [7.0870],\n",
       "        [5.4880],\n",
       "        [5.5460],\n",
       "        [6.5960],\n",
       "        [7.5260],\n",
       "        [5.1550],\n",
       "        [5.9760],\n",
       "        [4.3620],\n",
       "        [6.0680],\n",
       "        [5.5170],\n",
       "        [4.5080],\n",
       "        [7.4130],\n",
       "        [6.4780],\n",
       "        [4.1210],\n",
       "        [4.2520],\n",
       "        [6.9940],\n",
       "        [4.2760],\n",
       "        [5.0330],\n",
       "        [6.3240],\n",
       "        [3.6070],\n",
       "        [4.0280],\n",
       "        [4.8710],\n",
       "        [5.4580],\n",
       "        [5.1450],\n",
       "        [7.5010],\n",
       "        [4.4040],\n",
       "        [5.3140],\n",
       "        [4.8130],\n",
       "        [4.5750],\n",
       "        [6.9070],\n",
       "        [7.2670],\n",
       "        [5.9770],\n",
       "        [3.9160],\n",
       "        [5.5100],\n",
       "        [5.9210],\n",
       "        [5.3030],\n",
       "        [5.9190],\n",
       "        [4.3560],\n",
       "        [5.4010],\n",
       "        [6.2390],\n",
       "        [5.1850],\n",
       "        [4.8760],\n",
       "        [5.5600],\n",
       "        [5.1290],\n",
       "        [3.6220],\n",
       "        [5.6150],\n",
       "        [5.8130],\n",
       "        [6.8710],\n",
       "        [5.1210],\n",
       "        [3.6950],\n",
       "        [4.1560],\n",
       "        [6.0050],\n",
       "        [4.0730],\n",
       "        [6.4880],\n",
       "        [4.2010],\n",
       "        [5.6480],\n",
       "        [6.7780],\n",
       "        [5.8970],\n",
       "        [4.9070],\n",
       "        [5.1610],\n",
       "        [5.1510],\n",
       "        [4.3950],\n",
       "        [4.5740],\n",
       "        [4.7930],\n",
       "        [7.3390],\n",
       "        [7.3340],\n",
       "        [5.9920],\n",
       "        [3.8560],\n",
       "        [4.8750],\n",
       "        [5.7710],\n",
       "        [7.4980],\n",
       "        [5.1320],\n",
       "        [4.7540],\n",
       "        [6.7010],\n",
       "        [5.5380],\n",
       "        [5.7430],\n",
       "        [5.2790],\n",
       "        [5.8350],\n",
       "        [5.1230],\n",
       "        [7.0390],\n",
       "        [6.3750],\n",
       "        [5.5280],\n",
       "        [5.8560],\n",
       "        [3.5150],\n",
       "        [6.3790],\n",
       "        [4.2190],\n",
       "        [5.1770],\n",
       "        [4.6350],\n",
       "        [6.7390],\n",
       "        [6.0780],\n",
       "        [5.7680],\n",
       "        [5.4400],\n",
       "        [5.0570],\n",
       "        [4.4590],\n",
       "        [5.8350],\n",
       "        [3.8320],\n",
       "        [6.3610],\n",
       "        [4.4150],\n",
       "        [4.1390],\n",
       "        [6.2690],\n",
       "        [7.2910],\n",
       "        [7.5090],\n",
       "        [3.0690],\n",
       "        [6.3790],\n",
       "        [4.9960],\n",
       "        [3.6660],\n",
       "        [6.4740],\n",
       "        [3.3030],\n",
       "        [6.1680],\n",
       "        [5.0450],\n",
       "        [5.3890],\n",
       "        [5.6580],\n",
       "        [3.7390],\n",
       "        [4.3240],\n",
       "        [6.5730],\n",
       "        [6.7250],\n",
       "        [7.1040],\n",
       "        [6.5450],\n",
       "        [5.9870],\n",
       "        [6.0840],\n",
       "        [5.0610],\n",
       "        [3.7240],\n",
       "        [4.7950],\n",
       "        [4.1930],\n",
       "        [3.7940],\n",
       "        [4.6440],\n",
       "        [5.8720],\n",
       "        [3.7950],\n",
       "        [6.5990],\n",
       "        [4.3760],\n",
       "        [7.2840],\n",
       "        [7.0060],\n",
       "        [5.2340],\n",
       "        [6.0870],\n",
       "        [4.6080],\n",
       "        [5.5690],\n",
       "        [6.8910],\n",
       "        [5.9560],\n",
       "        [3.6570],\n",
       "        [5.0110],\n",
       "        [5.8230],\n",
       "        [5.1820],\n",
       "        [3.7660],\n",
       "        [6.6350],\n",
       "        [4.7140],\n",
       "        [4.0320],\n",
       "        [2.9050],\n",
       "        [4.1680],\n",
       "        [4.6950],\n",
       "        [7.3160],\n",
       "        [2.6930],\n",
       "        [3.9360],\n",
       "        [6.6520],\n",
       "        [5.2730],\n",
       "        [6.3570],\n",
       "        [4.2910],\n",
       "        [4.2800],\n",
       "        [7.0790],\n",
       "        [5.2930],\n",
       "        [5.6210],\n",
       "        [6.6090],\n",
       "        [7.5220],\n",
       "        [5.2300],\n",
       "        [6.0080],\n",
       "        [4.7350],\n",
       "        [6.0030],\n",
       "        [5.6110],\n",
       "        [4.4600],\n",
       "        [7.4690],\n",
       "        [6.4420],\n",
       "        [4.4650],\n",
       "        [4.2860],\n",
       "        [6.9510],\n",
       "        [4.1200],\n",
       "        [5.2270],\n",
       "        [6.4540],\n",
       "        [3.5070],\n",
       "        [3.6030],\n",
       "        [5.1810],\n",
       "        [5.3240],\n",
       "        [7.5040],\n",
       "        [4.3150],\n",
       "        [5.2620],\n",
       "        [4.6920],\n",
       "        [4.4970],\n",
       "        [6.9770],\n",
       "        [7.2130],\n",
       "        [5.9640],\n",
       "        [4.1800],\n",
       "        [5.3110],\n",
       "        [5.9200],\n",
       "        [5.3360],\n",
       "        [5.8190],\n",
       "        [4.5530],\n",
       "        [5.2790],\n",
       "        [6.1050],\n",
       "        [5.0040],\n",
       "        [5.8500],\n",
       "        [5.2250],\n",
       "        [3.8080],\n",
       "        [3.5330],\n",
       "        [5.5250],\n",
       "        [5.9020],\n",
       "        [6.8630],\n",
       "        [5.1750],\n",
       "        [3.6440],\n",
       "        [3.9700],\n",
       "        [6.0840],\n",
       "        [4.1900],\n",
       "        [6.5270],\n",
       "        [4.2920],\n",
       "        [5.6290],\n",
       "        [6.5780],\n",
       "        [5.8380],\n",
       "        [4.9550],\n",
       "        [5.2370],\n",
       "        [5.2350],\n",
       "        [4.5500],\n",
       "        [4.5450],\n",
       "        [4.5740],\n",
       "        [4.9620],\n",
       "        [7.3770],\n",
       "        [7.3140],\n",
       "        [6.0710],\n",
       "        [4.0280],\n",
       "        [5.0740],\n",
       "        [5.8100],\n",
       "        [7.5370],\n",
       "        [5.2690],\n",
       "        [4.7750],\n",
       "        [6.4520],\n",
       "        [5.4930],\n",
       "        [5.7150],\n",
       "        [5.4300],\n",
       "        [5.9730],\n",
       "        [5.1950],\n",
       "        [6.3750],\n",
       "        [5.8250],\n",
       "        [5.9630],\n",
       "        [3.4710],\n",
       "        [6.3440],\n",
       "        [4.5350],\n",
       "        [5.3950],\n",
       "        [4.7090],\n",
       "        [6.5720],\n",
       "        [6.0980],\n",
       "        [5.7580],\n",
       "        [5.1510],\n",
       "        [4.8290],\n",
       "        [5.8380],\n",
       "        [3.5910],\n",
       "        [6.4030],\n",
       "        [4.4400],\n",
       "        [4.1390],\n",
       "        [7.2840],\n",
       "        [7.4940],\n",
       "        [3.4620],\n",
       "        [6.4220],\n",
       "        [5.0410],\n",
       "        [3.3490],\n",
       "        [6.4240],\n",
       "        [3.4950],\n",
       "        [6.1680],\n",
       "        [4.8050],\n",
       "        [5.5000],\n",
       "        [5.8220],\n",
       "        [4.0810],\n",
       "        [4.0960],\n",
       "        [6.6480],\n",
       "        [6.7140],\n",
       "        [6.9930],\n",
       "        [6.4540],\n",
       "        [5.9710],\n",
       "        [5.2500],\n",
       "        [5.0740],\n",
       "        [3.5930],\n",
       "        [4.5140],\n",
       "        [3.8750]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma mieć 469 obserwacje i jeden wymiar wynikowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([469, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\"> Sprawdzam czy wyniki tensorów są OK\n",
    "    \n",
    "    Mamy identyczny układ jak w dataTable DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor zmiennych opisujących: torch.Size([469, 3])\n",
      "Tensor zmiennych wynikowych:  torch.Size([469, 1])\n"
     ]
    }
   ],
   "source": [
    "print('Tensor zmiennych opisujących:', X.size())\n",
    "print('Tensor zmiennych wynikowych: ',y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\">  JEDEN REKORD, który podstawimy do modelu gdy będzie gotowy.\n",
    "    \n",
    "    aby sprawdzić czy model dobrze liczy\n",
    "\n",
    "#### Wartość xPredicted\n",
    "\n",
    "Zmienna xPredicted jest pojedynczym wejściem (jednym krajem), dla którego chcemy przewidzieć ocenę z wykorzystaniem parametrów wyuczonych przez sieć neuronową."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor zmiennych do prognozy:  torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "xPredicted = torch.tensor(([0.0954, 0.3364, 0.3758]), dtype=torch.float,device=device)\n",
    "print('Tensor zmiennych do prognozy: ',xPredicted.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0954, 0.3364, 0.3758], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xPredicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sprawdzam czy wymiary tensorów są OK\n",
    "\n",
    "Tensor zmiennych opisujących   : torch.Size([22, 4])\n",
    "\n",
    "Tensor zmiennych wynikowych    : torch.Size([22, 1])\n",
    "\n",
    "Tensor 1 obserwacji do prognozy: torch.Size([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor zmiennych opisujących   : torch.Size([469, 3])\n",
      "Tensor zmiennych wynikowych    : torch.Size([469, 1])\n",
      "Tensor 1 obserwacji do prognozy: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print('Tensor zmiennych opisujących   :',X.size())\n",
    "print('Tensor zmiennych wynikowych    :',y.size())\n",
    "print('Tensor 1 obserwacji do prognozy:',xPredicted.size())"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalowanie i normalizacja\n",
    "Dane przetwarzane przez sieć pochodzą najczęściej z obserwacji pewnych wartości w badanym modelu. Ich skala wartości zazwyczaj nie na bezpośrednie wprowadzenie na wejścia sieci. Istnieje kilka popularnych metod skalowania: \n",
    "\n",
    "skalowanie względem wartości maksymalnej: \n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalowanie wartości X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1761, 0.3496, 0.1710],\n",
      "        [0.1162, 0.5335, 0.4697],\n",
      "        [0.3150, 0.4267, 0.5021],\n",
      "        ...,\n",
      "        [0.1029, 0.3725, 0.3163],\n",
      "        [0.1417, 0.6892, 0.3402],\n",
      "        [0.1728, 0.5023, 0.2009]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# scale units\n",
    "X_max, _ = torch.max(X, 0)\n",
    "X = torch.div(X, X_max)\n",
    "\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalowanie wartości y i wartości, pod którą szukamy prognozy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2539, 0.8952, 1.0000], device='cuda:0')\n",
      "tensor([[0.0357],\n",
      "        [0.0496],\n",
      "        [0.0560],\n",
      "        [0.0403],\n",
      "        [0.0657],\n",
      "        [0.0435],\n",
      "        [0.0728],\n",
      "        [0.0720],\n",
      "        [0.0521],\n",
      "        [0.0596],\n",
      "        [0.0469],\n",
      "        [0.0581],\n",
      "        [0.0694],\n",
      "        [0.0334],\n",
      "        [0.0525],\n",
      "        [0.0589],\n",
      "        [0.0495],\n",
      "        [0.0433],\n",
      "        [0.0698],\n",
      "        [0.0422],\n",
      "        [0.0359],\n",
      "        [0.0290],\n",
      "        [0.0382],\n",
      "        [0.0425],\n",
      "        [0.0743],\n",
      "        [0.0368],\n",
      "        [0.0367],\n",
      "        [0.0667],\n",
      "        [0.0514],\n",
      "        [0.0648],\n",
      "        [0.0396],\n",
      "        [0.0399],\n",
      "        [0.0452],\n",
      "        [0.0723],\n",
      "        [0.0576],\n",
      "        [0.0569],\n",
      "        [0.0650],\n",
      "        [0.0753],\n",
      "        [0.0437],\n",
      "        [0.0488],\n",
      "        [0.0597],\n",
      "        [0.0419],\n",
      "        [0.0613],\n",
      "        [0.0543],\n",
      "        [0.0451],\n",
      "        [0.0741],\n",
      "        [0.0657],\n",
      "        [0.0390],\n",
      "        [0.0430],\n",
      "        [0.0675],\n",
      "        [0.0463],\n",
      "        [0.0486],\n",
      "        [0.0612],\n",
      "        [0.0366],\n",
      "        [0.0452],\n",
      "        [0.0479],\n",
      "        [0.0547],\n",
      "        [0.0480],\n",
      "        [0.0756],\n",
      "        [0.0456],\n",
      "        [0.0540],\n",
      "        [0.0469],\n",
      "        [0.0468],\n",
      "        [0.0694],\n",
      "        [0.0728],\n",
      "        [0.0595],\n",
      "        [0.0366],\n",
      "        [0.0571],\n",
      "        [0.0599],\n",
      "        [0.0519],\n",
      "        [0.0586],\n",
      "        [0.0442],\n",
      "        [0.0559],\n",
      "        [0.0630],\n",
      "        [0.0529],\n",
      "        [0.0488],\n",
      "        [0.0510],\n",
      "        [0.0484],\n",
      "        [0.0490],\n",
      "        [0.0457],\n",
      "        [0.0575],\n",
      "        [0.0583],\n",
      "        [0.0695],\n",
      "        [0.0501],\n",
      "        [0.0368],\n",
      "        [0.0429],\n",
      "        [0.0577],\n",
      "        [0.0399],\n",
      "        [0.0630],\n",
      "        [0.0444],\n",
      "        [0.0548],\n",
      "        [0.0719],\n",
      "        [0.0589],\n",
      "        [0.0487],\n",
      "        [0.0519],\n",
      "        [0.0501],\n",
      "        [0.0497],\n",
      "        [0.0431],\n",
      "        [0.0451],\n",
      "        [0.0738],\n",
      "        [0.0729],\n",
      "        [0.0583],\n",
      "        [0.0384],\n",
      "        [0.0527],\n",
      "        [0.0569],\n",
      "        [0.0752],\n",
      "        [0.0685],\n",
      "        [0.0519],\n",
      "        [0.0472],\n",
      "        [0.0679],\n",
      "        [0.0588],\n",
      "        [0.0582],\n",
      "        [0.0507],\n",
      "        [0.0579],\n",
      "        [0.0510],\n",
      "        [0.0661],\n",
      "        [0.0512],\n",
      "        [0.0572],\n",
      "        [0.0346],\n",
      "        [0.0641],\n",
      "        [0.0390],\n",
      "        [0.0512],\n",
      "        [0.0451],\n",
      "        [0.0680],\n",
      "        [0.0599],\n",
      "        [0.0585],\n",
      "        [0.0506],\n",
      "        [0.0464],\n",
      "        [0.0598],\n",
      "        [0.0633],\n",
      "        [0.0427],\n",
      "        [0.0455],\n",
      "        [0.0627],\n",
      "        [0.0487],\n",
      "        [0.0736],\n",
      "        [0.0759],\n",
      "        [0.0301],\n",
      "        [0.0630],\n",
      "        [0.0479],\n",
      "        [0.0378],\n",
      "        [0.0645],\n",
      "        [0.0284],\n",
      "        [0.0617],\n",
      "        [0.0474],\n",
      "        [0.0533],\n",
      "        [0.0555],\n",
      "        [0.0393],\n",
      "        [0.0468],\n",
      "        [0.0690],\n",
      "        [0.0687],\n",
      "        [0.0712],\n",
      "        [0.0649],\n",
      "        [0.0600],\n",
      "        [0.0681],\n",
      "        [0.0536],\n",
      "        [0.0408],\n",
      "        [0.0513],\n",
      "        [0.0461],\n",
      "        [0.0336],\n",
      "        [0.0466],\n",
      "        [0.0635],\n",
      "        [0.0387],\n",
      "        [0.0665],\n",
      "        [0.0436],\n",
      "        [0.0731],\n",
      "        [0.0712],\n",
      "        [0.0529],\n",
      "        [0.0622],\n",
      "        [0.0464],\n",
      "        [0.0580],\n",
      "        [0.0693],\n",
      "        [0.0596],\n",
      "        [0.0348],\n",
      "        [0.0520],\n",
      "        [0.0582],\n",
      "        [0.0516],\n",
      "        [0.0397],\n",
      "        [0.0695],\n",
      "        [0.0422],\n",
      "        [0.0374],\n",
      "        [0.0290],\n",
      "        [0.0391],\n",
      "        [0.0451],\n",
      "        [0.0740],\n",
      "        [0.0376],\n",
      "        [0.0670],\n",
      "        [0.0524],\n",
      "        [0.0648],\n",
      "        [0.0396],\n",
      "        [0.0424],\n",
      "        [0.0427],\n",
      "        [0.0709],\n",
      "        [0.0549],\n",
      "        [0.0555],\n",
      "        [0.0660],\n",
      "        [0.0753],\n",
      "        [0.0516],\n",
      "        [0.0598],\n",
      "        [0.0436],\n",
      "        [0.0607],\n",
      "        [0.0552],\n",
      "        [0.0451],\n",
      "        [0.0741],\n",
      "        [0.0648],\n",
      "        [0.0412],\n",
      "        [0.0425],\n",
      "        [0.0699],\n",
      "        [0.0428],\n",
      "        [0.0503],\n",
      "        [0.0632],\n",
      "        [0.0361],\n",
      "        [0.0403],\n",
      "        [0.0487],\n",
      "        [0.0546],\n",
      "        [0.0514],\n",
      "        [0.0750],\n",
      "        [0.0440],\n",
      "        [0.0531],\n",
      "        [0.0481],\n",
      "        [0.0457],\n",
      "        [0.0691],\n",
      "        [0.0727],\n",
      "        [0.0598],\n",
      "        [0.0392],\n",
      "        [0.0551],\n",
      "        [0.0592],\n",
      "        [0.0530],\n",
      "        [0.0592],\n",
      "        [0.0436],\n",
      "        [0.0540],\n",
      "        [0.0624],\n",
      "        [0.0518],\n",
      "        [0.0488],\n",
      "        [0.0556],\n",
      "        [0.0513],\n",
      "        [0.0362],\n",
      "        [0.0561],\n",
      "        [0.0581],\n",
      "        [0.0687],\n",
      "        [0.0512],\n",
      "        [0.0369],\n",
      "        [0.0416],\n",
      "        [0.0600],\n",
      "        [0.0407],\n",
      "        [0.0649],\n",
      "        [0.0420],\n",
      "        [0.0565],\n",
      "        [0.0678],\n",
      "        [0.0590],\n",
      "        [0.0491],\n",
      "        [0.0516],\n",
      "        [0.0515],\n",
      "        [0.0439],\n",
      "        [0.0457],\n",
      "        [0.0479],\n",
      "        [0.0734],\n",
      "        [0.0733],\n",
      "        [0.0599],\n",
      "        [0.0386],\n",
      "        [0.0487],\n",
      "        [0.0577],\n",
      "        [0.0750],\n",
      "        [0.0513],\n",
      "        [0.0475],\n",
      "        [0.0670],\n",
      "        [0.0554],\n",
      "        [0.0574],\n",
      "        [0.0528],\n",
      "        [0.0584],\n",
      "        [0.0512],\n",
      "        [0.0704],\n",
      "        [0.0637],\n",
      "        [0.0553],\n",
      "        [0.0586],\n",
      "        [0.0351],\n",
      "        [0.0638],\n",
      "        [0.0422],\n",
      "        [0.0518],\n",
      "        [0.0464],\n",
      "        [0.0674],\n",
      "        [0.0608],\n",
      "        [0.0577],\n",
      "        [0.0544],\n",
      "        [0.0506],\n",
      "        [0.0446],\n",
      "        [0.0584],\n",
      "        [0.0383],\n",
      "        [0.0636],\n",
      "        [0.0441],\n",
      "        [0.0414],\n",
      "        [0.0627],\n",
      "        [0.0729],\n",
      "        [0.0751],\n",
      "        [0.0307],\n",
      "        [0.0638],\n",
      "        [0.0500],\n",
      "        [0.0367],\n",
      "        [0.0647],\n",
      "        [0.0330],\n",
      "        [0.0617],\n",
      "        [0.0505],\n",
      "        [0.0539],\n",
      "        [0.0566],\n",
      "        [0.0374],\n",
      "        [0.0432],\n",
      "        [0.0657],\n",
      "        [0.0672],\n",
      "        [0.0710],\n",
      "        [0.0654],\n",
      "        [0.0599],\n",
      "        [0.0608],\n",
      "        [0.0506],\n",
      "        [0.0372],\n",
      "        [0.0479],\n",
      "        [0.0419],\n",
      "        [0.0379],\n",
      "        [0.0464],\n",
      "        [0.0587],\n",
      "        [0.0380],\n",
      "        [0.0660],\n",
      "        [0.0438],\n",
      "        [0.0728],\n",
      "        [0.0701],\n",
      "        [0.0523],\n",
      "        [0.0609],\n",
      "        [0.0461],\n",
      "        [0.0557],\n",
      "        [0.0689],\n",
      "        [0.0596],\n",
      "        [0.0366],\n",
      "        [0.0501],\n",
      "        [0.0582],\n",
      "        [0.0518],\n",
      "        [0.0377],\n",
      "        [0.0663],\n",
      "        [0.0471],\n",
      "        [0.0403],\n",
      "        [0.0290],\n",
      "        [0.0417],\n",
      "        [0.0470],\n",
      "        [0.0732],\n",
      "        [0.0269],\n",
      "        [0.0394],\n",
      "        [0.0665],\n",
      "        [0.0527],\n",
      "        [0.0636],\n",
      "        [0.0429],\n",
      "        [0.0428],\n",
      "        [0.0708],\n",
      "        [0.0529],\n",
      "        [0.0562],\n",
      "        [0.0661],\n",
      "        [0.0752],\n",
      "        [0.0523],\n",
      "        [0.0601],\n",
      "        [0.0474],\n",
      "        [0.0600],\n",
      "        [0.0561],\n",
      "        [0.0446],\n",
      "        [0.0747],\n",
      "        [0.0644],\n",
      "        [0.0446],\n",
      "        [0.0429],\n",
      "        [0.0695],\n",
      "        [0.0412],\n",
      "        [0.0523],\n",
      "        [0.0645],\n",
      "        [0.0351],\n",
      "        [0.0360],\n",
      "        [0.0518],\n",
      "        [0.0532],\n",
      "        [0.0750],\n",
      "        [0.0432],\n",
      "        [0.0526],\n",
      "        [0.0469],\n",
      "        [0.0450],\n",
      "        [0.0698],\n",
      "        [0.0721],\n",
      "        [0.0596],\n",
      "        [0.0418],\n",
      "        [0.0531],\n",
      "        [0.0592],\n",
      "        [0.0534],\n",
      "        [0.0582],\n",
      "        [0.0455],\n",
      "        [0.0528],\n",
      "        [0.0610],\n",
      "        [0.0500],\n",
      "        [0.0585],\n",
      "        [0.0522],\n",
      "        [0.0381],\n",
      "        [0.0353],\n",
      "        [0.0553],\n",
      "        [0.0590],\n",
      "        [0.0686],\n",
      "        [0.0518],\n",
      "        [0.0364],\n",
      "        [0.0397],\n",
      "        [0.0608],\n",
      "        [0.0419],\n",
      "        [0.0653],\n",
      "        [0.0429],\n",
      "        [0.0563],\n",
      "        [0.0658],\n",
      "        [0.0584],\n",
      "        [0.0495],\n",
      "        [0.0524],\n",
      "        [0.0523],\n",
      "        [0.0455],\n",
      "        [0.0454],\n",
      "        [0.0457],\n",
      "        [0.0496],\n",
      "        [0.0738],\n",
      "        [0.0731],\n",
      "        [0.0607],\n",
      "        [0.0403],\n",
      "        [0.0507],\n",
      "        [0.0581],\n",
      "        [0.0754],\n",
      "        [0.0527],\n",
      "        [0.0477],\n",
      "        [0.0645],\n",
      "        [0.0549],\n",
      "        [0.0571],\n",
      "        [0.0543],\n",
      "        [0.0597],\n",
      "        [0.0520],\n",
      "        [0.0637],\n",
      "        [0.0582],\n",
      "        [0.0596],\n",
      "        [0.0347],\n",
      "        [0.0634],\n",
      "        [0.0453],\n",
      "        [0.0539],\n",
      "        [0.0471],\n",
      "        [0.0657],\n",
      "        [0.0610],\n",
      "        [0.0576],\n",
      "        [0.0515],\n",
      "        [0.0483],\n",
      "        [0.0584],\n",
      "        [0.0359],\n",
      "        [0.0640],\n",
      "        [0.0444],\n",
      "        [0.0414],\n",
      "        [0.0728],\n",
      "        [0.0749],\n",
      "        [0.0346],\n",
      "        [0.0642],\n",
      "        [0.0504],\n",
      "        [0.0335],\n",
      "        [0.0642],\n",
      "        [0.0349],\n",
      "        [0.0617],\n",
      "        [0.0480],\n",
      "        [0.0550],\n",
      "        [0.0582],\n",
      "        [0.0408],\n",
      "        [0.0410],\n",
      "        [0.0665],\n",
      "        [0.0671],\n",
      "        [0.0699],\n",
      "        [0.0645],\n",
      "        [0.0597],\n",
      "        [0.0525],\n",
      "        [0.0507],\n",
      "        [0.0359],\n",
      "        [0.0451],\n",
      "        [0.0388]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
    "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
    "y = y / 100  # max test score is 100\n",
    "print(xPredicted)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINIOWANIE MODELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # parameters\n",
    "        # TODO: parameters can be parameterized instead of declaring them here\n",
    "        self.inputSize = 3   # ---WARSTWA WEJŚCIOWA -- Tyle ile jest obserwacji (kolumn po trnspozycji macierzy X)\n",
    "        self.outputSize = 1  # ---WARSTWA WYJŚCIOWA -- Tyle ile jest Y (tu mamy 1 kolumnę)\n",
    "        self.hiddenSize = 3  # --WARSTWA UKRYTA ---Tyle ile jest obserwacji (kolumn po trnspozycji macierzy X)\n",
    "        \n",
    "        # weights\n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize,device=device) # 3 X 55 tensor\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize,device=device) # 55 X 1 tensor\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.z = torch.matmul(X, self.W1) # 3 X 55\".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def saveWeights(self, model):\n",
    "        # we will use the PyTorch internal storage functions\n",
    "        torch.save(model, \"NN\")\n",
    "        # you can reload model with all the weights and so forth with:\n",
    "        # torch.load(\"NN\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening\n",
    "Pozostało już tylko trenować sieć neuronową. Najpierw tworzymy instancję wykresu obliczeniowego, który właśnie zbudowaliśmy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([469, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([469, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie trenujemy model do 150 rund. Zauważ, że w PyTorch NN(X) automatycznie wywołuje funkcję forward, więc nie ma potrzeby jawnego wywoływania NN.forward(X).\n",
    "\n",
    "Po uzyskaniu przewidywanego wyniku dla każdej rundy szkolenia, obliczamy stratę za pomocą następującego kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11228933185338974"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y - NN(X))**2).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.4878762662410736\n",
      "#1 Loss: 0.0030135137494653463\n",
      "#2 Loss: 0.0030135137494653463\n",
      "#3 Loss: 0.0030135137494653463\n",
      "#4 Loss: 0.0030135137494653463\n",
      "#5 Loss: 0.0030135137494653463\n",
      "#6 Loss: 0.0030135137494653463\n",
      "#7 Loss: 0.0030135137494653463\n",
      "#8 Loss: 0.0030135137494653463\n",
      "#9 Loss: 0.0030135137494653463\n",
      "#10 Loss: 0.0030135137494653463\n",
      "#11 Loss: 0.0030135137494653463\n",
      "#12 Loss: 0.0030135137494653463\n",
      "#13 Loss: 0.0030135137494653463\n",
      "#14 Loss: 0.0030135137494653463\n",
      "#15 Loss: 0.0030135137494653463\n",
      "#16 Loss: 0.0030135137494653463\n",
      "#17 Loss: 0.0030135137494653463\n",
      "#18 Loss: 0.0030135137494653463\n",
      "#19 Loss: 0.0030135137494653463\n",
      "#20 Loss: 0.0030135137494653463\n",
      "#21 Loss: 0.0030135137494653463\n",
      "#22 Loss: 0.0030135137494653463\n",
      "#23 Loss: 0.0030135137494653463\n",
      "#24 Loss: 0.0030135137494653463\n",
      "#25 Loss: 0.0030135137494653463\n",
      "#26 Loss: 0.0030135137494653463\n",
      "#27 Loss: 0.0030135137494653463\n",
      "#28 Loss: 0.0030135137494653463\n",
      "#29 Loss: 0.0030135137494653463\n",
      "#30 Loss: 0.0030135137494653463\n",
      "#31 Loss: 0.0030135137494653463\n",
      "#32 Loss: 0.0030135137494653463\n",
      "#33 Loss: 0.0030135137494653463\n",
      "#34 Loss: 0.0030135137494653463\n",
      "#35 Loss: 0.0030135137494653463\n",
      "#36 Loss: 0.0030135137494653463\n",
      "#37 Loss: 0.0030135137494653463\n",
      "#38 Loss: 0.0030135137494653463\n",
      "#39 Loss: 0.0030135137494653463\n",
      "#40 Loss: 0.0030135137494653463\n",
      "#41 Loss: 0.0030135137494653463\n",
      "#42 Loss: 0.0030135137494653463\n",
      "#43 Loss: 0.0030135137494653463\n",
      "#44 Loss: 0.0030135137494653463\n",
      "#45 Loss: 0.0030135137494653463\n",
      "#46 Loss: 0.0030135137494653463\n",
      "#47 Loss: 0.0030135137494653463\n",
      "#48 Loss: 0.0030135137494653463\n",
      "#49 Loss: 0.0030135137494653463\n",
      "#50 Loss: 0.0030135137494653463\n",
      "#51 Loss: 0.0030135137494653463\n",
      "#52 Loss: 0.0030135137494653463\n",
      "#53 Loss: 0.0030135137494653463\n",
      "#54 Loss: 0.0030135137494653463\n",
      "#55 Loss: 0.0030135137494653463\n",
      "#56 Loss: 0.0030135137494653463\n",
      "#57 Loss: 0.0030135137494653463\n",
      "#58 Loss: 0.0030135137494653463\n",
      "#59 Loss: 0.0030135137494653463\n",
      "#60 Loss: 0.0030135137494653463\n",
      "#61 Loss: 0.0030135137494653463\n",
      "#62 Loss: 0.0030135137494653463\n",
      "#63 Loss: 0.0030135137494653463\n",
      "#64 Loss: 0.0030135137494653463\n",
      "#65 Loss: 0.0030135137494653463\n",
      "#66 Loss: 0.0030135137494653463\n",
      "#67 Loss: 0.0030135137494653463\n",
      "#68 Loss: 0.0030135137494653463\n",
      "#69 Loss: 0.0030135137494653463\n",
      "#70 Loss: 0.0030135137494653463\n",
      "#71 Loss: 0.0030135137494653463\n",
      "#72 Loss: 0.0030135137494653463\n",
      "#73 Loss: 0.0030135137494653463\n",
      "#74 Loss: 0.0030135137494653463\n",
      "#75 Loss: 0.0030135137494653463\n",
      "#76 Loss: 0.0030135137494653463\n",
      "#77 Loss: 0.0030135137494653463\n",
      "#78 Loss: 0.0030135137494653463\n",
      "#79 Loss: 0.0030135137494653463\n",
      "#80 Loss: 0.0030135137494653463\n",
      "#81 Loss: 0.0030135137494653463\n",
      "#82 Loss: 0.0030135137494653463\n",
      "#83 Loss: 0.0030135137494653463\n",
      "#84 Loss: 0.0030135137494653463\n",
      "#85 Loss: 0.0030135137494653463\n",
      "#86 Loss: 0.0030135137494653463\n",
      "#87 Loss: 0.0030135137494653463\n",
      "#88 Loss: 0.0030135137494653463\n",
      "#89 Loss: 0.0030135137494653463\n",
      "#90 Loss: 0.0030135137494653463\n",
      "#91 Loss: 0.0030135137494653463\n",
      "#92 Loss: 0.0030135137494653463\n",
      "#93 Loss: 0.0030135137494653463\n",
      "#94 Loss: 0.0030135137494653463\n",
      "#95 Loss: 0.0030135137494653463\n",
      "#96 Loss: 0.0030135137494653463\n",
      "#97 Loss: 0.0030135137494653463\n",
      "#98 Loss: 0.0030135137494653463\n",
      "#99 Loss: 0.0030135137494653463\n",
      "#100 Loss: 0.0030135137494653463\n",
      "#101 Loss: 0.0030135137494653463\n",
      "#102 Loss: 0.0030135137494653463\n",
      "#103 Loss: 0.0030135137494653463\n",
      "#104 Loss: 0.0030135137494653463\n",
      "#105 Loss: 0.0030135137494653463\n",
      "#106 Loss: 0.0030135137494653463\n",
      "#107 Loss: 0.0030135137494653463\n",
      "#108 Loss: 0.0030135137494653463\n",
      "#109 Loss: 0.0030135137494653463\n",
      "#110 Loss: 0.0030135137494653463\n",
      "#111 Loss: 0.0030135137494653463\n",
      "#112 Loss: 0.0030135137494653463\n",
      "#113 Loss: 0.0030135137494653463\n",
      "#114 Loss: 0.0030135137494653463\n",
      "#115 Loss: 0.0030135137494653463\n",
      "#116 Loss: 0.0030135137494653463\n",
      "#117 Loss: 0.0030135137494653463\n",
      "#118 Loss: 0.0030135137494653463\n",
      "#119 Loss: 0.0030135137494653463\n",
      "#120 Loss: 0.0030135137494653463\n",
      "#121 Loss: 0.0030135137494653463\n",
      "#122 Loss: 0.0030135137494653463\n",
      "#123 Loss: 0.0030135137494653463\n",
      "#124 Loss: 0.0030135137494653463\n",
      "#125 Loss: 0.0030135137494653463\n",
      "#126 Loss: 0.0030135137494653463\n",
      "#127 Loss: 0.0030135137494653463\n",
      "#128 Loss: 0.0030135137494653463\n",
      "#129 Loss: 0.0030135137494653463\n",
      "#130 Loss: 0.0030135137494653463\n",
      "#131 Loss: 0.0030135137494653463\n",
      "#132 Loss: 0.0030135137494653463\n",
      "#133 Loss: 0.0030135137494653463\n",
      "#134 Loss: 0.0030135137494653463\n",
      "#135 Loss: 0.0030135137494653463\n",
      "#136 Loss: 0.0030135137494653463\n",
      "#137 Loss: 0.0030135137494653463\n",
      "#138 Loss: 0.0030135137494653463\n",
      "#139 Loss: 0.0030135137494653463\n",
      "#140 Loss: 0.0030135137494653463\n",
      "#141 Loss: 0.0030135137494653463\n",
      "#142 Loss: 0.0030135137494653463\n",
      "#143 Loss: 0.0030135137494653463\n",
      "#144 Loss: 0.0030135137494653463\n",
      "#145 Loss: 0.0030135137494653463\n",
      "#146 Loss: 0.0030135137494653463\n",
      "#147 Loss: 0.0030135137494653463\n",
      "#148 Loss: 0.0030135137494653463\n",
      "#149 Loss: 0.0030135137494653463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wojciech/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Neural_Network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "for i in range(150):  # trains the NN 1500 times\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(X, y)\n",
    "NN.saveWeights(NN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:#1155cc\"> Podstawienie do modelu rekordu, który chcieliśmy sprawdzić"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self):\n",
    "    print (\"Predicted data based on trained weights: \")\n",
    "    print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "    print (\"Output: \\n\" + str(self.forward(xPredicted)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "tensor([0.2539, 0.8952, 1.0000], device='cuda:0')\n",
      "Output: \n",
      "tensor([1.0134e-23], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predict(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 180%;color:red\">Sprawdźmy czy dobrze policzy Afganistan 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Economy (GDP per Capita)</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Trust (Government Corruption)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.401477</td>\n",
       "      <td>0.10618</td>\n",
       "      <td>0.061158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Economy (GDP per Capita)  Freedom  Trust (Government Corruption)\n",
       "330                  0.401477  0.10618                       0.061158"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST = df[(df['Country']=='Afghanistan')&(df['Year']==2017)]\n",
    "TEST[['Economy (GDP per Capita)','Freedom','Trust (Government Corruption)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330    0.401477\n",
       "Name: Economy (GDP per Capita), dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST['Economy (GDP per Capita)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330    0.10618\n",
       "Name: Freedom, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST['Freedom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330    0.061158\n",
       "Name: Trust (Government Corruption), dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST['Trust (Government Corruption)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor zmiennych do prognozy:  torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x_Test = torch.tensor(([0.401477, 0.10618, 0.061158]), dtype=torch.float,device=device)\n",
    "print('Tensor zmiennych do prognozy: ',x_Test.size())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# scale units\n",
    "X_max, _ = torch.max(x_Test, 0)\n",
    "x_Test = torch.div(x_Test, X_max)\n",
    "\n",
    "\n",
    "print(x_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRansponuje wektor zmiennych niezależnych aby stał się kolumną"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self,f):\n",
    "    print (\"Predicted data based on trained weights: \")\n",
    "    print (\"Input (scaled): \\n\" + str(f))\n",
    "    print (\"Output: \\n\" + str(self.forward(f)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "tensor([0.4015, 0.1062, 0.0612], device='cuda:0')\n",
      "Output: \n",
      "tensor([2.0684e-22], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predict(NN,x_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odpowiedź prawidłowa brzmi: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330    3.794\n",
       "Name: Happiness Score, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['Country']=='Afghanistan')&(df['Year']==2017)]['Happiness Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5750],\n",
       "        [4.9590],\n",
       "        [5.6050],\n",
       "        [4.0330],\n",
       "        [6.5740],\n",
       "        [4.3500],\n",
       "        [7.2840],\n",
       "        [7.2000],\n",
       "        [5.2120],\n",
       "        [5.9600],\n",
       "        [4.6940],\n",
       "        [5.8130],\n",
       "        [6.9370],\n",
       "        [3.3400],\n",
       "        [5.2530],\n",
       "        [5.8900],\n",
       "        [4.9490],\n",
       "        [4.3320],\n",
       "        [6.9830],\n",
       "        [4.2180],\n",
       "        [3.5870],\n",
       "        [2.9050],\n",
       "        [3.8190],\n",
       "        [4.2520],\n",
       "        [7.4270],\n",
       "        [3.6780],\n",
       "        [3.6670],\n",
       "        [6.6700],\n",
       "        [5.1400],\n",
       "        [6.4770],\n",
       "        [3.9560],\n",
       "        [3.9890],\n",
       "        [4.5170],\n",
       "        [7.2260],\n",
       "        [5.7590],\n",
       "        [5.6890],\n",
       "        [6.5050],\n",
       "        [7.5270],\n",
       "        [4.3690],\n",
       "        [4.8850],\n",
       "        [5.9750],\n",
       "        [4.1940],\n",
       "        [6.1300],\n",
       "        [5.4290],\n",
       "        [4.5120],\n",
       "        [7.4060],\n",
       "        [6.5750],\n",
       "        [3.8960],\n",
       "        [4.2970],\n",
       "        [6.7500],\n",
       "        [4.6330],\n",
       "        [4.8570],\n",
       "        [6.1230],\n",
       "        [3.6560],\n",
       "        [4.5180],\n",
       "        [4.7880],\n",
       "        [5.4740],\n",
       "        [4.8000],\n",
       "        [7.5610],\n",
       "        [4.5650],\n",
       "        [5.3990],\n",
       "        [4.6860],\n",
       "        [4.6770],\n",
       "        [6.9400],\n",
       "        [7.2780],\n",
       "        [5.9480],\n",
       "        [3.6550],\n",
       "        [5.7090],\n",
       "        [5.9870],\n",
       "        [5.1920],\n",
       "        [5.8550],\n",
       "        [4.4190],\n",
       "        [5.5890],\n",
       "        [6.2950],\n",
       "        [5.2860],\n",
       "        [4.8760],\n",
       "        [5.0980],\n",
       "        [4.8390],\n",
       "        [4.8980],\n",
       "        [4.5710],\n",
       "        [5.7540],\n",
       "        [5.8330],\n",
       "        [6.9460],\n",
       "        [5.0070],\n",
       "        [3.6810],\n",
       "        [4.2920],\n",
       "        [5.7700],\n",
       "        [3.9950],\n",
       "        [6.3020],\n",
       "        [4.4360],\n",
       "        [5.4770],\n",
       "        [7.1870],\n",
       "        [5.8890],\n",
       "        [4.8740],\n",
       "        [5.1920],\n",
       "        [5.0130],\n",
       "        [4.9710],\n",
       "        [4.3070],\n",
       "        [4.5140],\n",
       "        [7.3780],\n",
       "        [7.2860],\n",
       "        [5.8280],\n",
       "        [3.8450],\n",
       "        [5.2680],\n",
       "        [5.6950],\n",
       "        [7.5220],\n",
       "        [6.8530],\n",
       "        [5.1940],\n",
       "        [4.7150],\n",
       "        [6.7860],\n",
       "        [5.8780],\n",
       "        [5.8240],\n",
       "        [5.0730],\n",
       "        [5.7910],\n",
       "        [5.1020],\n",
       "        [6.6110],\n",
       "        [5.1240],\n",
       "        [5.7160],\n",
       "        [3.4650],\n",
       "        [6.4110],\n",
       "        [3.9040],\n",
       "        [5.1230],\n",
       "        [4.5070],\n",
       "        [6.7980],\n",
       "        [5.9950],\n",
       "        [5.8480],\n",
       "        [5.0570],\n",
       "        [4.6420],\n",
       "        [5.9840],\n",
       "        [6.3290],\n",
       "        [4.2710],\n",
       "        [4.5500],\n",
       "        [6.2690],\n",
       "        [4.8670],\n",
       "        [7.3640],\n",
       "        [7.5870],\n",
       "        [3.0060],\n",
       "        [6.2980],\n",
       "        [4.7860],\n",
       "        [3.7810],\n",
       "        [6.4550],\n",
       "        [2.8390],\n",
       "        [6.1680],\n",
       "        [4.7390],\n",
       "        [5.3320],\n",
       "        [5.5480],\n",
       "        [3.9310],\n",
       "        [4.6810],\n",
       "        [6.9010],\n",
       "        [6.8670],\n",
       "        [7.1190],\n",
       "        [6.4850],\n",
       "        [6.0030],\n",
       "        [6.8100],\n",
       "        [5.3600],\n",
       "        [4.0770],\n",
       "        [5.1290],\n",
       "        [4.6100],\n",
       "        [3.3600],\n",
       "        [4.6550],\n",
       "        [6.3550],\n",
       "        [3.8660],\n",
       "        [6.6500],\n",
       "        [4.3600],\n",
       "        [7.3130],\n",
       "        [7.1190],\n",
       "        [5.2910],\n",
       "        [6.2180],\n",
       "        [4.6430],\n",
       "        [5.8020],\n",
       "        [6.9290],\n",
       "        [5.9560],\n",
       "        [3.4840],\n",
       "        [5.1960],\n",
       "        [5.8220],\n",
       "        [5.1630],\n",
       "        [3.9740],\n",
       "        [6.9520],\n",
       "        [4.2170],\n",
       "        [3.7390],\n",
       "        [2.9050],\n",
       "        [3.9070],\n",
       "        [4.5130],\n",
       "        [7.4040],\n",
       "        [3.7630],\n",
       "        [6.7050],\n",
       "        [5.2450],\n",
       "        [6.4810],\n",
       "        [3.9560],\n",
       "        [4.2360],\n",
       "        [4.2720],\n",
       "        [7.0870],\n",
       "        [5.4880],\n",
       "        [5.5460],\n",
       "        [6.5960],\n",
       "        [7.5260],\n",
       "        [5.1550],\n",
       "        [5.9760],\n",
       "        [4.3620],\n",
       "        [6.0680],\n",
       "        [5.5170],\n",
       "        [4.5080],\n",
       "        [7.4130],\n",
       "        [6.4780],\n",
       "        [4.1210],\n",
       "        [4.2520],\n",
       "        [6.9940],\n",
       "        [4.2760],\n",
       "        [5.0330],\n",
       "        [6.3240],\n",
       "        [3.6070],\n",
       "        [4.0280],\n",
       "        [4.8710],\n",
       "        [5.4580],\n",
       "        [5.1450],\n",
       "        [7.5010],\n",
       "        [4.4040],\n",
       "        [5.3140],\n",
       "        [4.8130],\n",
       "        [4.5750],\n",
       "        [6.9070],\n",
       "        [7.2670],\n",
       "        [5.9770],\n",
       "        [3.9160],\n",
       "        [5.5100],\n",
       "        [5.9210],\n",
       "        [5.3030],\n",
       "        [5.9190],\n",
       "        [4.3560],\n",
       "        [5.4010],\n",
       "        [6.2390],\n",
       "        [5.1850],\n",
       "        [4.8760],\n",
       "        [5.5600],\n",
       "        [5.1290],\n",
       "        [3.6220],\n",
       "        [5.6150],\n",
       "        [5.8130],\n",
       "        [6.8710],\n",
       "        [5.1210],\n",
       "        [3.6950],\n",
       "        [4.1560],\n",
       "        [6.0050],\n",
       "        [4.0730],\n",
       "        [6.4880],\n",
       "        [4.2010],\n",
       "        [5.6480],\n",
       "        [6.7780],\n",
       "        [5.8970],\n",
       "        [4.9070],\n",
       "        [5.1610],\n",
       "        [5.1510],\n",
       "        [4.3950],\n",
       "        [4.5740],\n",
       "        [4.7930],\n",
       "        [7.3390],\n",
       "        [7.3340],\n",
       "        [5.9920],\n",
       "        [3.8560],\n",
       "        [4.8750],\n",
       "        [5.7710],\n",
       "        [7.4980],\n",
       "        [5.1320],\n",
       "        [4.7540],\n",
       "        [6.7010],\n",
       "        [5.5380],\n",
       "        [5.7430],\n",
       "        [5.2790],\n",
       "        [5.8350],\n",
       "        [5.1230],\n",
       "        [7.0390],\n",
       "        [6.3750],\n",
       "        [5.5280],\n",
       "        [5.8560],\n",
       "        [3.5150],\n",
       "        [6.3790],\n",
       "        [4.2190],\n",
       "        [5.1770],\n",
       "        [4.6350],\n",
       "        [6.7390],\n",
       "        [6.0780],\n",
       "        [5.7680],\n",
       "        [5.4400],\n",
       "        [5.0570],\n",
       "        [4.4590],\n",
       "        [5.8350],\n",
       "        [3.8320],\n",
       "        [6.3610],\n",
       "        [4.4150],\n",
       "        [4.1390],\n",
       "        [6.2690],\n",
       "        [7.2910],\n",
       "        [7.5090],\n",
       "        [3.0690],\n",
       "        [6.3790],\n",
       "        [4.9960],\n",
       "        [3.6660],\n",
       "        [6.4740],\n",
       "        [3.3030],\n",
       "        [6.1680],\n",
       "        [5.0450],\n",
       "        [5.3890],\n",
       "        [5.6580],\n",
       "        [3.7390],\n",
       "        [4.3240],\n",
       "        [6.5730],\n",
       "        [6.7250],\n",
       "        [7.1040],\n",
       "        [6.5450],\n",
       "        [5.9870],\n",
       "        [6.0840],\n",
       "        [5.0610],\n",
       "        [3.7240],\n",
       "        [4.7950],\n",
       "        [4.1930],\n",
       "        [3.7940],\n",
       "        [4.6440],\n",
       "        [5.8720],\n",
       "        [3.7950],\n",
       "        [6.5990],\n",
       "        [4.3760],\n",
       "        [7.2840],\n",
       "        [7.0060],\n",
       "        [5.2340],\n",
       "        [6.0870],\n",
       "        [4.6080],\n",
       "        [5.5690],\n",
       "        [6.8910],\n",
       "        [5.9560],\n",
       "        [3.6570],\n",
       "        [5.0110],\n",
       "        [5.8230],\n",
       "        [5.1820],\n",
       "        [3.7660],\n",
       "        [6.6350],\n",
       "        [4.7140],\n",
       "        [4.0320],\n",
       "        [2.9050],\n",
       "        [4.1680],\n",
       "        [4.6950],\n",
       "        [7.3160],\n",
       "        [2.6930],\n",
       "        [3.9360],\n",
       "        [6.6520],\n",
       "        [5.2730],\n",
       "        [6.3570],\n",
       "        [4.2910],\n",
       "        [4.2800],\n",
       "        [7.0790],\n",
       "        [5.2930],\n",
       "        [5.6210],\n",
       "        [6.6090],\n",
       "        [7.5220],\n",
       "        [5.2300],\n",
       "        [6.0080],\n",
       "        [4.7350],\n",
       "        [6.0030],\n",
       "        [5.6110],\n",
       "        [4.4600],\n",
       "        [7.4690],\n",
       "        [6.4420],\n",
       "        [4.4650],\n",
       "        [4.2860],\n",
       "        [6.9510],\n",
       "        [4.1200],\n",
       "        [5.2270],\n",
       "        [6.4540],\n",
       "        [3.5070],\n",
       "        [3.6030],\n",
       "        [5.1810],\n",
       "        [5.3240],\n",
       "        [7.5040],\n",
       "        [4.3150],\n",
       "        [5.2620],\n",
       "        [4.6920],\n",
       "        [4.4970],\n",
       "        [6.9770],\n",
       "        [7.2130],\n",
       "        [5.9640],\n",
       "        [4.1800],\n",
       "        [5.3110],\n",
       "        [5.9200],\n",
       "        [5.3360],\n",
       "        [5.8190],\n",
       "        [4.5530],\n",
       "        [5.2790],\n",
       "        [6.1050],\n",
       "        [5.0040],\n",
       "        [5.8500],\n",
       "        [5.2250],\n",
       "        [3.8080],\n",
       "        [3.5330],\n",
       "        [5.5250],\n",
       "        [5.9020],\n",
       "        [6.8630],\n",
       "        [5.1750],\n",
       "        [3.6440],\n",
       "        [3.9700],\n",
       "        [6.0840],\n",
       "        [4.1900],\n",
       "        [6.5270],\n",
       "        [4.2920],\n",
       "        [5.6290],\n",
       "        [6.5780],\n",
       "        [5.8380],\n",
       "        [4.9550],\n",
       "        [5.2370],\n",
       "        [5.2350],\n",
       "        [4.5500],\n",
       "        [4.5450],\n",
       "        [4.5740],\n",
       "        [4.9620],\n",
       "        [7.3770],\n",
       "        [7.3140],\n",
       "        [6.0710],\n",
       "        [4.0280],\n",
       "        [5.0740],\n",
       "        [5.8100],\n",
       "        [7.5370],\n",
       "        [5.2690],\n",
       "        [4.7750],\n",
       "        [6.4520],\n",
       "        [5.4930],\n",
       "        [5.7150],\n",
       "        [5.4300],\n",
       "        [5.9730],\n",
       "        [5.1950],\n",
       "        [6.3750],\n",
       "        [5.8250],\n",
       "        [5.9630],\n",
       "        [3.4710],\n",
       "        [6.3440],\n",
       "        [4.5350],\n",
       "        [5.3950],\n",
       "        [4.7090],\n",
       "        [6.5720],\n",
       "        [6.0980],\n",
       "        [5.7580],\n",
       "        [5.1510],\n",
       "        [4.8290],\n",
       "        [5.8380],\n",
       "        [3.5910],\n",
       "        [6.4030],\n",
       "        [4.4400],\n",
       "        [4.1390],\n",
       "        [7.2840],\n",
       "        [7.4940],\n",
       "        [3.4620],\n",
       "        [6.4220],\n",
       "        [5.0410],\n",
       "        [3.3490],\n",
       "        [6.4240],\n",
       "        [3.4950],\n",
       "        [6.1680],\n",
       "        [4.8050],\n",
       "        [5.5000],\n",
       "        [5.8220],\n",
       "        [4.0810],\n",
       "        [4.0960],\n",
       "        [6.6480],\n",
       "        [6.7140],\n",
       "        [6.9930],\n",
       "        [6.4540],\n",
       "        [5.9710],\n",
       "        [5.2500],\n",
       "        [5.0740],\n",
       "        [3.5930],\n",
       "        [4.5140],\n",
       "        [3.8750]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
